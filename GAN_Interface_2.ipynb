{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and TF setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras \n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from scipy import signal, fftpack\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import UpSampling2D, LeakyReLU, Lambda, Add, Multiply, Activation, Conv2DTranspose\n",
    "from keras.layers import Cropping2D, ZeroPadding2D, Flatten, Subtract\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from functools import partial\n",
    "from New_Layers import *\n",
    "from keras.layers.merge import _Merge\n",
    "from multiprocessing import Pool\n",
    "import copy \n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import astropy\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "from astropy.io.fits.card import UNDEFINED\n",
    "from astropy.io import fits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "# GAN Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, generator, discriminator, training_scheme, \n",
    "                 generator_kwargs={}, discriminator_kwargs={}, \n",
    "                 generator_training_kwargs={}, discriminator_training_kwargs={}):\n",
    "        \n",
    "        assert training_scheme is not None , \"No training scheme selected!\"\n",
    "        assert isinstance(generator, keras.models.Model), \"Generator is not a model!\"\n",
    "        assert isinstance(discriminator, keras.models.Model), \"Discriminator is not a model!\"\n",
    "        \n",
    "        assert type(generator_kwargs) is dict , \"generator kwargs are not a dictionary!\"\n",
    "        assert type(discriminator_kwargs) is dict , \"discriminator kwargs are not a dictionary!\"\n",
    "        assert type(generator_training_kwargs) is dict , \"discriminator training kwargs are not a dictionary!\"\n",
    "        assert type(discriminator_training_kwargs) is dict , \"generator training kwargs are not a dictionary!\"\n",
    "        \n",
    "        \n",
    "        self._training_scheme = training_scheme\n",
    "\n",
    "        self.__generator_model = generator\n",
    "        self.__discriminator_model = discriminator\n",
    "\n",
    "        self.__discriminator = self._training_scheme.compile_discriminator(self.__generator_model, \n",
    "                                                                           self.__discriminator_model, \n",
    "                                                                           **discriminator_kwargs)\n",
    "        self.__generator = self._training_scheme.compile_generator(self.__generator_model, \n",
    "                                                                   self.__discriminator_model,\n",
    "                                                                   **generator_kwargs)\n",
    "        self.generator_training_kwargs = generator_training_kwargs\n",
    "        self.discriminator_training_kwargs = discriminator_training_kwargs\n",
    "    \n",
    "    def generator_model(self): return self.__generator_model\n",
    "    def discriminator_model(self): return self.__discriminator_model\n",
    "    def generator(self): return self.__generator\n",
    "    def discriminator(self): return self.__discriminator\n",
    "    def summaries(self):\n",
    "        print \"\\n\\n\\nGenerator Summary: \\n\"\n",
    "        self.__generator_model.summary()\n",
    "        plot_model(self.__generator_model, show_shapes=True, to_file='GAN_Generator_Model.png')\n",
    "        \n",
    "        print \"\\n\\n\\nDiscriminator Summary: \\n\"\n",
    "        self.__discriminator_model.summary()\n",
    "        plot_model(self.__discriminator_model, show_shapes=True, to_file='GAN_Discriminator_Model.png')\n",
    "        \n",
    "        print \"\\n\\n\\nGenerator Training Model Summary: \\n\"\n",
    "        self.__generator.summary()\n",
    "        plot_model(self.__generator, show_shapes=True, to_file='GAN_Generator_Training_Model.png')\n",
    "       \n",
    "        print \"\\n\\n\\nDiscriminator Training Model Summary: \\n\"\n",
    "        self.__discriminator.summary()\n",
    "        plot_model(self.__discriminator, show_shapes=True, to_file='GAN_Discriminator_Training_Model.png')\n",
    "   \n",
    "    def fit(self, x, y, verbose=False, shuffle=False, steps=None, epochs=None, steps_per_epoch=None, batch_size=None, \n",
    "            generator_training_multiplier=1, discriminator_training_multiplier=1, \n",
    "            generator_callbacks=[],discriminator_callbacks=[], **kwargs):\n",
    "        self.verbose = verbose\n",
    "        self.callbacks_generator, self.callbacks_discriminator= [], []\n",
    "        self.History = keras.callbacks.History()\n",
    "        self.shuffle = shuffle\n",
    "        assert (steps is None and epochs is not None and steps_per_epoch is not None) or \\\n",
    "               (steps is not None and epochs is None and steps_per_epoch is None), \"please supply either steps OR epochs and steps per epoch\"\n",
    "        \n",
    "        assert batch_size is not None, \"batch size is None, please provide batch size\"\n",
    "        try:\n",
    "            iterator = iter(generator_callbacks)            \n",
    "        except TypeError:\n",
    "            assert False, \"generator callbacks are not iterable!\"\n",
    "        \n",
    "        try:\n",
    "            iterator = iter(discriminator_callbacks)            \n",
    "        except TypeError:\n",
    "            assert False, \"discriminator callbacks are not iterable!\"\n",
    "        \n",
    "        for c in generator_callbacks:\n",
    "            c.set_model(self.__generator)\n",
    "            self.callbacks_generator.append(c)\n",
    "        \n",
    "        for c in discriminator_callbacks:\n",
    "            c.set_model(self.__discriminator)\n",
    "            self.callbacks_discriminator.append(c)\n",
    "        \n",
    "        \n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator + [self.History]:\n",
    "            callback.on_train_begin()\n",
    "        \n",
    "        \n",
    "        if steps is not None:\n",
    "            for i in tqdm(xrange(steps)):\n",
    "                    temp_loss_discriminator, temp_loss_generator = self.__fit_on_batch(x=x, y=y, step=i, \n",
    "                                                                                       shuffle=self.shuffle,\n",
    "                                                                                       batch_size=batch_size, \n",
    "                                                                                       generator_training_multiplier=generator_training_multiplier,\n",
    "                                                                                       discriminator_training_multiplier=discriminator_training_multiplier)\n",
    "                    loss_discriminator = {('discriminator_'+self.__discriminator.metrics_names[i]):item for i,item in enumerate(temp_loss_discriminator)}\n",
    "                    loss_generator = {('generator_'+self.__generator.metrics_names[i]):item for i,item in enumerate(temp_loss_generator)}\n",
    "                    losses = loss_discriminator.copy().update(loss_generator)\n",
    "                    self.History.on_epoch_end(i,losses) # populate history\n",
    "        elif steps_per_epoch is not None and epochs is not None:\n",
    "            for k in xrange(epochs):\n",
    "                for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "                    callback.on_epoch_begin(k)\n",
    "                loss_discriminator, loss_generator = None, None\n",
    "                for i in xrange(steps_per_epoch):\n",
    "                    temp_loss_discriminator, temp_loss_generator = self.__fit_on_batch(x=x, y=y, step=i, epoch=k, \n",
    "                                                                                       shuffle=self.shuffle,\n",
    "                                                                                       steps_per_epoch=steps_per_epoch, \n",
    "                                                                                       batch_size=batch_size,\n",
    "                                                                                       generator_training_multiplier=generator_training_multiplier,\n",
    "                                                                                       discriminator_training_multiplier=discriminator_training_multiplier)\n",
    "\n",
    "                    if loss_discriminator is None: loss_discriminator = temp_loss_discriminator\n",
    "                    elif hasattr(loss_discriminator, '__iter__'): loss_discriminator = [x+y for x,y in zip(loss_discriminator, temp_loss_discriminator)]\n",
    "                    else: loss_discriminator += temp_loss_discriminator\n",
    "                        \n",
    "                    if loss_generator is None: loss_generator = temp_loss_generator\n",
    "                    elif hasattr(loss_generator, '__iter__'): loss_generator = [x+y for x,y in zip(loss_generator, temp_loss_generator)]\n",
    "                    else: loss_generator += temp_loss_generator\n",
    "                \n",
    "                loss_discriminator = [item * 1.0/steps_per_epoch for item in loss_discriminator]\n",
    "                loss_generator = [item * 1.0/steps_per_epoch for item in loss_generator]\n",
    "                                    \n",
    "                for callback in self.callbacks_generator:\n",
    "                    callback.on_epoch_end(k, logs={self.__generator.metrics_names[i]:item for i,item in enumerate(loss_generator)})\n",
    "\n",
    "                for callback in self.callbacks_discriminator:\n",
    "                    callback.on_epoch_end(k, logs={self.__discriminator.metrics_names[i]:item for i,item in enumerate(loss_discriminator)})\n",
    "\n",
    "                loss_discriminator = {('discriminator_'+self.__discriminator.metrics_names[i]):item for i,item in enumerate(loss_discriminator)}\n",
    "                loss_generator = {('generator_'+self.__generator.metrics_names[i]):item for i,item in enumerate(loss_generator)}\n",
    "                losses = loss_discriminator.copy().update(loss_generator)\n",
    "                self.History.on_epoch_end(k,losses) # populate history\n",
    "                    \n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "            callback.on_train_end()\n",
    "\n",
    "        return self.History\n",
    "\n",
    "\n",
    "    def __fit_on_batch(self, x, y, step, epoch=None, steps_per_epoch=None, generator_training_multiplier=1, \n",
    "                       discriminator_training_multiplier=1, batch_size=None, shuffle=False, **kwargs):\n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "                if epoch is not None:\n",
    "                    callback.on_batch_begin(step, logs={'batch':step, 'size':batch_size})\n",
    "                else:\n",
    "                    callback.on_epoch_begin(step)\n",
    "                    \n",
    "        steps = step if epoch is None else step + (epoch * steps_per_epoch)\n",
    "        curr_x, curr_y = x, y\n",
    "\n",
    "        # Discriminator Training\n",
    "        loss = None\n",
    "        for j in range(discriminator_training_multiplier): \n",
    "            curr = (steps*discriminator_training_multiplier + j) * batch_size\n",
    "            nex = curr + batch_size\n",
    "            idxs = [i % curr_x.shape[0] for i in range(curr,nex)]\n",
    "            if shuffle: idxs = np.random.randint(0, curr_x.shape[0], size=batch_size)\n",
    "            train_x, train_y = curr_x[idxs], curr_y[idxs]\n",
    "            temp_loss = self._training_scheme.train_discriminator(self.__discriminator, train_x, train_y, batch_size, **self.discriminator_training_kwargs)\n",
    "            if loss is None: loss = temp_loss\n",
    "            elif hasattr(loss, '__iter__'): loss = [x+y for x,y in zip(loss, temp_loss)]\n",
    "            else: loss += temp_loss\n",
    "             \n",
    "        loss_discriminator = [item * 1.0/discriminator_training_multiplier for item in loss]\n",
    "        if self.verbose: print 'Discriminator Loss:', loss_discriminator\n",
    "        \n",
    "        # Generator Training\n",
    "        loss = None\n",
    "        for j in range(generator_training_multiplier): \n",
    "            curr = (steps*generator_training_multiplier + j) * batch_size\n",
    "            nex = curr + batch_size\n",
    "            idxs = [i % curr_x.shape[0] for i in range(curr,nex)]\n",
    "            if shuffle: idxs = np.random.randint(0, curr_x.shape[0], size=batch_size)\n",
    "            train_x, train_y = curr_x[idxs], curr_y[idxs]\n",
    "            temp_loss = self._training_scheme.train_generator(self.__generator, train_x, train_y, batch_size, **self.generator_training_kwargs)\n",
    "            if loss is None: loss = temp_loss\n",
    "            elif hasattr(loss, '__iter__'): loss = [x+y for x,y in zip(loss, temp_loss)]\n",
    "            else: loss += temp_loss\n",
    "            \n",
    "        loss_generator = [item * 1.0/generator_training_multiplier for item in loss]\n",
    "        if self.verbose: print 'Generator Loss:', loss_generator\n",
    "\n",
    "        for callback in self.callbacks_generator: # callbacks for generator\n",
    "            if epoch is not None:\n",
    "                callback.on_batch_end(step, logs={self.__generator.metrics_names[i]:item for i,item in enumerate(loss_generator)})\n",
    "            else:\n",
    "                callback.on_epoch_end(step, logs={self.__generator.metrics_names[i]:item for i,item in enumerate(loss_generator)})\n",
    "            \n",
    "        for callback in self.callbacks_discriminator: # callbacks for discriminator\n",
    "            if epoch is not None:\n",
    "                callback.on_batch_end(step, logs={self.__discriminator.metrics_names[i]:item for i,item in enumerate(loss_discriminator)})\n",
    "            else:\n",
    "                callback.on_epoch_end(step, logs={self.__discriminator.metrics_names[i]:item for i,item in enumerate(loss_discriminator)})\n",
    "\n",
    "        return loss_discriminator, loss_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# IWGAN TrainingScheme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_TrainingScheme(object):\n",
    "    @staticmethod\n",
    "    def compile_discriminator(generator, discriminator, **kwargs):\n",
    "            raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_generator(generator, discriminator, **kwargs):\n",
    "            raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    @staticmethod\n",
    "    def train_discriminator(discriminator, x, y, batch_size, **kwargs):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_generator(generator, x, y, batch_size, **kwargs):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "    \n",
    "class IWGAN_TrainingScheme(Base_TrainingScheme):\n",
    "\n",
    "    @staticmethod\n",
    "    def wasserstein_loss(y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_penalty_loss(y_true, y_pred, averaged_samples, gradient_penalty_weight):\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "    @staticmethod\n",
    "    class RandomWeightedAverage(_Merge):\n",
    "        def __init__(self, batch_size,**kwargs):\n",
    "            super(IWGAN_TrainingScheme.RandomWeightedAverage, self).__init__(**kwargs)\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "        def _merge_function(self, inputs):\n",
    "            weights = K.random_uniform((self.batch_size, 1, 1, 1))\n",
    "            return (weights * inputs[0]) + ((1 - weights) * inputs[1])\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_discriminator(generator, discriminator, optimizer, batch_size, **kwargs):\n",
    "        gp_weight = 10\n",
    "        for layer in generator.layers: layer.trainable = False\n",
    "        generator.trainable = False\n",
    "        inp = Input(tuple(generator.layers[0].input_shape[1:]))\n",
    "        in_gen = generator(inp)\n",
    "        in_real = Input(tuple(discriminator.layers[0].input_shape[1:]))\n",
    "        discriminator_output_from_generator = discriminator(in_gen)\n",
    "        discriminator_output_from_real_samples = discriminator(in_real)\n",
    "        averaged_samples = IWGAN_TrainingScheme.RandomWeightedAverage(batch_size)([in_real, in_gen])\n",
    "        averaged_samples_out = discriminator(averaged_samples)\n",
    "        partial_gp_loss = partial(IWGAN_TrainingScheme.gradient_penalty_loss, averaged_samples=averaged_samples,\n",
    "                                  gradient_penalty_weight=gp_weight)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "        in_gen.trainable = False\n",
    "\n",
    "        # ----- Discriminator -----\n",
    "        discriminator_model = Model(inputs=[in_real, inp], outputs=[discriminator_output_from_real_samples,\n",
    "                                                                    discriminator_output_from_generator,\n",
    "                                                                    averaged_samples_out])\n",
    "        discriminator_model.layers[1].trainable = False\n",
    "        discriminator_model.compile(optimizer=optimizer, loss=[IWGAN_TrainingScheme.wasserstein_loss,\n",
    "                                                               IWGAN_TrainingScheme.wasserstein_loss, partial_gp_loss])\n",
    "        # ----- Discriminator -----\n",
    "\n",
    "        for layer in generator.layers: layer.trainable = True\n",
    "        generator.trainable = True\n",
    "        return discriminator_model\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_generator(generator, discriminator, gen_loss, dis_loss, optimizer, \n",
    "                          gen_metrics, dis_metrics, gen_dis_loss_ratio, **kwargs):\n",
    "\n",
    "        \n",
    "        for layer in discriminator.layers: layer.trainable = False\n",
    "        discriminator.trainable = False\n",
    "        inp = Input(tuple(generator.layers[0].input_shape[1:]))\n",
    "        gen_out = generator(inp)\n",
    "\n",
    "        # ----- Generator -----\n",
    "        model = Model(inp, [gen_out,discriminator(generator(inp))])\n",
    "        model.layers[2].trainable = False\n",
    "        model.compile(loss={'discriminator': dis_loss, 'generator': gen_loss},\n",
    "                      optimizer= optimizer, metrics={'discriminator':dis_metrics, 'generator':gen_metrics},\n",
    "                      loss_weights={'discriminator': 1-gen_dis_loss_ratio, 'generator':gen_dis_loss_ratio})\n",
    "        # ----- Generator -----\n",
    "\n",
    "        for layer in discriminator.layers: layer.trainable = True\n",
    "        discriminator.trainable = True\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def train_discriminator(discriminator, x, y, batch_size, **kwargs):\n",
    "        # Discriminator Training\n",
    "        loss = discriminator.train_on_batch([y, x],  # inp_real, x\n",
    "                                            [np.ones((batch_size, 1), dtype=np.float32),  # discriminator_output_from_real_samples\n",
    "                                            -np.ones((batch_size, 1), dtype=np.float32),  # discriminator_output_from_generator\n",
    "                                            np.zeros((batch_size, 1), dtype=np.float32)])  # averaged_samples_out\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def train_generator(generator, x, y, batch_size, **kwargs):\n",
    "        # Generator Training\n",
    "        loss = generator.train_on_batch(x, [y, np.ones((batch_size, 1), dtype=np.float32)])            #x, [y , 1]\n",
    "                                 \n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Callbacks and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier_training(keras.callbacks.Callback):\n",
    "    def __init__(self, classifier_model, training_ratio, batch_size):\n",
    "        self.classifier_model = classifier_model\n",
    "        self.train_ratio = training_ratio\n",
    "        self.x, _ ,self.y = load_simulation_data()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss = None\n",
    "        for i in xrange(self.train_ratio):\n",
    "            idxs = np.random.randint(0, self.x.shape[0], size=self.batch_size)\n",
    "            loss = self.classifier_model.train_on_batch(self.x[idxs], self.y[idxs])\n",
    "        #print 'class loss: ', loss\n",
    "\n",
    "class log_results(keras.callbacks.Callback):\n",
    "    def __init__(self, wgan_model=None, classifier_model=None, logging_frequency=0, log_path=''):\n",
    "        self.logging_frequency = logging_frequency\n",
    "        self.log_path = log_path\n",
    "        self.model = wgan_model\n",
    "        self.classifier_model = classifier_model\n",
    "        self.last_best_classifier, self.last_best_generator, self.last_best_combined= 0, 0, 0\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.logging_frequency != 0 and epoch % self.logging_frequency == 0:\n",
    "            time = np.linspace(0, 28.625, 20610)\n",
    "            plt.figure(1, figsize=(10,10))\n",
    "            plt.subplot(211)\n",
    "            plt.scatter(time,  self.y[:1][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Simulation Output, \" + str(epoch))\n",
    "\n",
    "            final_res = self.model.generator().predict_on_batch(self.x[:1])\n",
    "            plt.subplot(212)\n",
    "            plt.scatter(time, final_res[0][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Neural Net Output,\"+ str(final_res[1][0,0]))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('img_'+str(epoch)+'_test.png')\n",
    "            plt.show()\n",
    "            self.get_score(epoch, self.model != None, self.classifier_model != None)\n",
    "    \n",
    "    \n",
    "    def get_score(self, i_in, gen_test = False, class_test = False):\n",
    "        def find_nearest(array, value):\n",
    "            array = np.asarray(array)\n",
    "            idx = (np.abs(array - value)).argmin()\n",
    "            return idx\n",
    "        \n",
    "        at_2perc = 1\n",
    "        if gen_test:\n",
    "            periods = np.load('../Data/total_params_sim_test_true_3.npy')[:,1]\n",
    "            transits_ref = np.load('../Data/total_transits_sim_test_true_3.npy')\n",
    "            x_test = np.expand_dims(np.load('../Data/total_x_sim_test_true_3.npy'),axis=1)\n",
    "            x_test = np.pad(x_test, ((0,0), (0,0) ,(0, 30+ 96), (0,0)), 'constant', constant_values=(0, 0))\n",
    "            print 'X loaded'\n",
    "            transits = self.model.generator().predict(x_test, verbose=1)[0][:, 0, :20610, :]\n",
    "            print \"Finished predicting data\"\n",
    "            transits_ref = transits_ref[:,:,0]\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaler.fit(transits_ref)\n",
    "            transits_ref = scaler.transform(transits_ref)\n",
    "            print(\"Finished loading data\")\n",
    "            periods = np.power(10, periods)\n",
    "            period_pred = []\n",
    "            model_preds = []\n",
    "            np.warnings.filterwarnings('ignore')\n",
    "            for i in tqdm(range(10000)): model_preds.append([transits[i, :, 0], periods[i], 1000, transits_ref[i, :]])\n",
    "            model_preds = np.asarray(imap_unordered_bar(process_transit, model_preds, 5))\n",
    "            auc_p, percentages, epsilon_range = p_epsilon_chart(model_preds[:, 0], model_preds[:, 1])\n",
    "            at_1perc = percentages[np.argmin(np.abs(epsilon_range - 0.01))]\n",
    "            at_2perc = percentages[np.argmin(np.abs(epsilon_range - 0.02))]\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(epsilon_range, percentages, label='Keras (area = {:.5f})'.format(auc_p))\n",
    "            plt.xlabel('epsilon')\n",
    "            plt.ylabel('period detection rate')\n",
    "            plt.title('Period ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xlim(0, 0.1)\n",
    "            plt.savefig('img_PAUC_'+str(i_in)+'_test.png')\n",
    "            plt.show()\n",
    "\n",
    "            if(at_2perc > self.last_best_generator): \n",
    "                self.model.generator().save('best_generator_test.h5')\n",
    "                self.classifier_model.save('best_classifier_test_generator_based.h5')\n",
    "                self.last_best_generator = at_2perc\n",
    "\n",
    "            if not class_test:\n",
    "                open(self.log_path,'a').write( 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc))\n",
    "                print 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc)\n",
    "\n",
    "        if class_test:\n",
    "            y_test = np.load('../Data/total_params_sim_test_3.npy')[:,1] > 0\n",
    "            x_test = np.expand_dims(np.load('../Data/total_x_sim_test_3.npy'),axis=1)\n",
    "            x_test = np.pad(x_test, ((0,0), (0,0) ,(0, 30+ 96), (0,0)), 'constant', constant_values=(0, 0))\n",
    "            y_pred = self.classifier_model.predict(x_test, verbose=1)[:,0]\n",
    "            fpr, tpr, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "            if(tpr[find_nearest(fpr, 0.01)] > self.last_best_classifier): \n",
    "                self.model.generator().save('best_generator_test_classifier_based.h5')\n",
    "                self.classifier_model.save('best_classifier_test.h5')\n",
    "                self.last_best_classifier = tpr[find_nearest(fpr, 0.01)]\n",
    "            \n",
    "            if(tpr[find_nearest(fpr, 0.01)] * at_2perc > self.last_best_combined):\n",
    "                self.model.generator().save('best_generator_combined_test.h5')\n",
    "                self.classifier_model.save('best_classifier_combined_test.h5')\n",
    "                self.last_best_combined = tpr[find_nearest(fpr, 0.01)] * at_2perc              \n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(fpr, tpr, label='Keras (area = {:.5f})'.format(roc_auc))\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.title('Classifier ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xlim(0, 0.02)\n",
    "            plt.savefig('img_ClassAUC_'+str(i_in)+'_test.png')\n",
    "            plt.show()\n",
    "\n",
    "            if not gen_test:\n",
    "                open(self.log_path,'a').write( 'i: %d, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc))\n",
    "                print 'i: %d, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc)\n",
    "                \n",
    "        if class_test and gen_test:\n",
    "            open(self.log_path,'a').write( 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc))\n",
    "            print 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc)\n",
    "\n",
    "class RocAucMetricCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, predict_batch_size=80, include_on_batch=False):\n",
    "        super(RocAucMetricCallback, self).__init__()\n",
    "        self.predict_batch_size = predict_batch_size\n",
    "        self.include_on_batch = include_on_batch\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('roc_auc_val' in self.params['metrics']):\n",
    "            self.params['metrics'].append('roc_auc_val')\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['roc_auc_val'] = float('-inf')\n",
    "        if (self.validation_data):\n",
    "            y_pred = self.model.predict(self.validation_data[0])[:, 0]\n",
    "            y_test = self.validation_data[1]\n",
    "            fpr, tpr, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "            logs['roc_auc_val'] = roc_auc_score(y_test, y_pred)\n",
    "            print(logs['roc_auc_val'])\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(fpr, tpr, label='Keras (area = {:.5f})'.format(logs['roc_auc_val']))\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.title('ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xlim(0, 0.02)\n",
    "            plt.show()\n",
    "            def find_nearest(array, value):\n",
    "                array = np.asarray(array)\n",
    "                idx = (np.abs(array - value)).argmin()\n",
    "                return idx\n",
    "            print 'Percentage at 0.001: %.5f, Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], logs['roc_auc_val'])\n",
    "\n",
    "class log_results_multi(keras.callbacks.Callback):\n",
    "    def __init__(self, logging_frequency=0, log_path='', x=None, y=None, \n",
    "                 myslice=slice(0,1), network_name='', save_path='best_generator_test_sectors.h5', image_name=''):\n",
    "        self.LOGDIR = 'TrainingLogs'\n",
    "        self.network_name = network_name\n",
    "        if not os.path.exists(self.LOGDIR): os.makedirs(self.LOGDIR)\n",
    "        if not os.path.exists(os.path.join(self.LOGDIR, self.network_name)): os.makedirs(os.path.join(self.LOGDIR, self.network_name))\n",
    "        self.logging_frequency = logging_frequency\n",
    "        self.log_path = log_path\n",
    "        self.model = None\n",
    "        self.save_path = save_path\n",
    "        self.x_test, self.y_test = x, y \n",
    "        self.last_best_generator= 0\n",
    "        self.myslice = myslice\n",
    "        self.image_name = image_name\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.logging_frequency != 0 and epoch % self.logging_frequency == 0:\n",
    "            time = np.linspace(0, 28.625, 20610)\n",
    "            plt.figure(1, figsize=(15,10))\n",
    "            plt.subplot(311)\n",
    "            plt.scatter(time,  self.x_test[self.myslice][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Simulation Output, \" + str(epoch))\n",
    "\n",
    "            plt.subplot(312)\n",
    "            plt.scatter(time,  self.y_test[self.myslice][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Simulation Output, \" + str(epoch))\n",
    "\n",
    "            final_res = self.model.predict_on_batch(self.x_test[self.myslice])\n",
    "            plt.subplot(313)\n",
    "            plt.scatter(time, final_res[0][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Neural Net Output,\"+ str(final_res[1][0,0]))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.LOGDIR, self.network_name, 'img_' + str(epoch)+ '_' + self.image_name +'_test.png'))\n",
    "            #plt.show()\n",
    "            plt.clf()\n",
    "            plt.close()\n",
    "            print \"epoch: %d current pic dice coeff: %f\" %(epoch,dice_coef(self.y_test[self.myslice][0, 0, :20610, 0], final_res[0][0, 0, :20610, 0]))\n",
    "            self.get_score(epoch, self.model != None)\n",
    "    \n",
    "    \n",
    "    def get_score(self, i_in, gen_test = False):\n",
    "        def find_nearest(array, value):\n",
    "            array = np.asarray(array)\n",
    "            idx = (np.abs(array - value)).argmin()\n",
    "            return idx\n",
    "        \n",
    "        if gen_test:\n",
    "            transits = self.model.predict(self.x_test, verbose=1)[0][:,0,:20610,0]\n",
    "            transits_ref = self.y_test[:,0,:20610,0]\n",
    "            print(transits.shape, transits_ref.shape)\n",
    "            np.warnings.filterwarnings('ignore')\n",
    "            dice_coeff = dice_coef(transits_ref, transits)\n",
    "            \n",
    "            if(dice_coeff > self.last_best_generator): \n",
    "                self.model.save(os.path.join(self.LOGDIR, self.network_name, self.save_path))\n",
    "                self.last_best_generator = dice_coeff \n",
    "            open(os.path.join(self.LOGDIR, self.network_name, self.log_path),'a').write('epoch: %d ,dice coef: %f\\n' % (i_in, dice_coeff))\n",
    "            print('epoch: %d ,dice coef: %f\\n' % (i_in, dice_coeff))\n",
    "\n",
    "            \n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def dice_coef(y_true,y_pred):\n",
    "    y_pred = y_pred > 0.01\n",
    "    oyt = np.sum((2.0 * y_true * y_pred)) / np.sum(y_true+y_pred)\n",
    "    print(oyt, np.sum((y_true * y_pred)), np.sum(y_true), np.sum(y_pred))\n",
    "    return oyt#sum((2 * y_true * y_pred))/ sum(y_true+y_pred)\n",
    "\n",
    "def p_epsilon_chart(p_test, p_pred):\n",
    "    percentages = []\n",
    "    auc_p = 0\n",
    "    epsilon_range = np.linspace(0, 1, 10000)\n",
    "    for epsilon in epsilon_range:\n",
    "        current_correct = p_pred[np.abs(1 - (p_pred / p_test)) < epsilon]\n",
    "        percentages.append(float(current_correct.shape[0]) / float(p_pred.shape[0]))\n",
    "        auc_p += float(percentages[-1]) / 10000\n",
    "    return auc_p, percentages, epsilon_range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# General Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_loss(y_true, y_pred):\n",
    "    return 10 * absolute_true_error(y_true, y_pred) + 3 * intersection_true_error(y_true, y_pred) + absolute_false_error(y_true, y_pred) + intersection_false_error(y_true, y_pred) - dice_coef1(y_true, y_pred)  # + masked_mse(y_true, y_pred)/10\n",
    "\n",
    "def load_simulation_data(header='', only_true=False, cross_validation=False):\n",
    "    x = np.load('../Data/total_x_sim_train_3.npy')\n",
    "    x = np.expand_dims(x,axis=1)\n",
    "    x = np.pad(x, ((0, 0), (0, 0), (0, 126), (0, 0)), 'constant', constant_values=(0.0, 0.0))\n",
    "    print 'X loaded'\n",
    "    y = np.load('../Data/total_transits_sim_train_3.npy')\n",
    "    y = y[:, :, 0]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(y)\n",
    "    y = scaler.transform(y)\n",
    "    y = np.expand_dims(y, axis=2)\n",
    "    ya = y[:, :, 0]\n",
    "    ya[ya != 0] = 1\n",
    "    y = np.expand_dims(ya, axis=2)\n",
    "    y = np.expand_dims(y, axis=1)\n",
    "    y = np.pad(y, ((0, 0), (0, 0), (0, 30 + 96), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "    print y.shape\n",
    "    print 'Y loaded'\n",
    "    print(x.shape, y.shape)\n",
    "    has_transit = np.load('../Data/total_params_sim_train_3.npy')[:,1] != 0\n",
    "    print np.sum(has_transit), has_transit.shape\n",
    "    print 'Params loaded'\n",
    "    print(\"Finished Loading Data\")\n",
    "    if only_true:\n",
    "        x = x[has_transit]\n",
    "        y = y[has_transit]\n",
    "    if cross_validation:\n",
    "        return train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "    return x, y, has_transit\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Network Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block_v2(inx, filters, kernel, activation, pooling):\n",
    "    x = BatchNormalization()(inx) # full pre activation\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = se_block(x, filters, 2) # Squeeze and Excitation Network\n",
    "    \n",
    "    x_k = add([x, inx]) # Resnet\n",
    "    \n",
    "    x = BatchNormalization()(x_k)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), strides=(1,pooling), padding='same')(x)\n",
    "    #x = MaxPooling2D(pool_size=(1, pooling), padding='same')(x) # convolutional Pool\n",
    "    return x, x_k # UNET\n",
    "\n",
    "def residual_block_up_v2(inx, x_k, filters, kernel, activation, pooling):\n",
    "    x = BatchNormalization()(inx) # full pre activation\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = se_block(x, filters, 2) # Squeeze and Excitation Network\n",
    "    x = add([x, x_k, inx]) # UNET with Resnet\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2DTranspose(filters, (1, kernel), strides=(1,pooling), padding='same')(x)\n",
    "    #x = UpSampling2D(size=(1, pooling))(x) # convolutional Pool\n",
    "    return x\n",
    "\n",
    "def residual_block(inx, filters, kernel, activation, pooling):\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(inx)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = add([x, inx])\n",
    "    x_k = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, pooling), strides=(1, pooling))(x_k)\n",
    "    return x, x_k\n",
    "\n",
    "def residual_block_up(inx, x_k, filters, kernel, activation, pooling):\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(inx)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = add([x, x_k, inx])\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = UpSampling2D((1, pooling))(x)\n",
    "    return x\n",
    "\n",
    "def se_block(inx, ch, ratio=16):\n",
    "    x = GlobalAveragePooling2D()(inx)\n",
    "    x = Dense(ch//ratio, activation='relu')(x)\n",
    "    x = Dense(ch, activation='sigmoid')(x)\n",
    "    return Multiply()([inx, x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(input_shape):\n",
    "    gen_inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (1, 5), padding='same')(gen_inputs)\n",
    "    x, x_k_1 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_2 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_3 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_4 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    \n",
    "    \n",
    "    x, x_k_5 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    x, x_k_6 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    x, x_k_7 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    y, x_k_8 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    \n",
    "    x = BatchNormalization()(y)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    decoder = x\n",
    "    x = Conv2D(64, (1, 5), padding='same')(decoder)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    m1 = x\n",
    "\n",
    "    x = Conv2D(128, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    m2 = x \n",
    "\n",
    "    x = Conv2D(256, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(256, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "\n",
    "    x = concatenate([m2,x])\n",
    "    x = Conv2D(128, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = concatenate([m1,x])\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = residual_block_up_v2(x, y, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_8, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_7, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_6, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_5, 64, 5, 'relu', 2)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (1, 5), padding='same')(x)\n",
    "    \n",
    "    x = residual_block_up_v2(x, x_k_4, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_3, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_2, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_1, 32, 5, 'relu', 1)\n",
    "    \n",
    "    _, x = residual_block_v2(x, 32, 5, 'relu', 1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    gen_outputs = Conv2D(1, (1, 1), activation='sigmoid', padding='same', name='gen_output')(x)\n",
    "    return Model(gen_inputs, gen_outputs, name='generator')\n",
    "    \n",
    "def Generator_2(input_shape):\n",
    "    gen_inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (1, 5), padding='same')(gen_inputs)\n",
    "    x, x_k_1 = residual_block(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_2 = residual_block(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_3 = residual_block(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_4 = residual_block(x, 32, 5, 'relu', 2)\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    x, x_k_5 = residual_block(x, 64, 5, 'relu', 2)\n",
    "    x, x_k_6 = residual_block(x, 64, 5, 'relu', 2)\n",
    "    x, x_k_7 = residual_block(x, 64, 5, 'relu', 2)\n",
    "    y, x_k_8 = residual_block(x, 64, 5, 'relu', 2)\n",
    "    x = Conv2D(64, (1, 5), padding='same')(y)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    encoded = x\n",
    "\n",
    "    decoder = encoded\n",
    "    x = Conv2D(64, (1, 5), padding='same')(decoder)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    m1 = x\n",
    "\n",
    "    x = Conv2D(128, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    m2 = x \n",
    "\n",
    "    x = Conv2D(256, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(256, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "\n",
    "    x = concatenate([m2,x])\n",
    "    x = Conv2D(128, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = concatenate([m1,x])\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = residual_block_up(x, y, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_8, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_7, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_6, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_5, 64, 5, 'relu', 2)\n",
    "    x = Conv2D(32, (1, 5), padding='same')(x)\n",
    "    x = residual_block_up(x, x_k_4, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_3, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_2, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_1, 32, 5, 'relu', 1)\n",
    "    _, x = residual_block(x, 32, 5, 'relu', 1)\n",
    "    gen_outputs = Conv2D(1, (1, 1), activation='sigmoid', padding='same', name='gen_output')(x)\n",
    "    return Model(gen_inputs, gen_outputs, name='generator')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Discriminator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(input_shape):\n",
    "    dis_inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (1, 5), strides=(1,2), padding='same')(dis_inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(256, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    dis_outputs = Dense(1, kernel_initializer='he_normal', name='dis_output')(x)\n",
    "    return Model(dis_inputs, dis_outputs, name='discriminator')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Classifier Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Classifier(input_shape):\n",
    "    class_in = Input(shape=input_shape)\n",
    "    class_inputs = []\n",
    "    class_lengths = [32, 32, 32, 32, 64, 64, 64, 64]\n",
    "    for i in xrange(8):\n",
    "        class_inputs.append(Input(shape=(K.int_shape(gen_inputs)[1], K.int_shape(gen_inputs)[2]//(2**i), class_lengths[i])))\n",
    "    \n",
    "    z = Conv2D(32, (1, 5), padding='same')(class_in)\n",
    "    z = Add()([z, class_inputs[0]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 32, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[1]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 32, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[2]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 32, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[3]])\n",
    "\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Conv2D(64, (1, 5), padding='same')(z)\n",
    "\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[4]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[5]])\n",
    "    \n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[6]])\n",
    "    \n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[7]])\n",
    "    \n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    \n",
    "    z = BatchNormalization()(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Conv2D(128, (1, 5), kernel_initializer='he_normal', padding='same')(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = LeakyReLU(0.2)(z)\n",
    "    z = Dropout(0.25)(z)\n",
    "\n",
    "    z = Conv2D(256, (1, 5), kernel_initializer='he_normal', padding='same')(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = LeakyReLU(0.2)(z)\n",
    "    z = Dropout(0.25)(z)\n",
    "\n",
    "\n",
    "    z = GlobalMaxPooling2D()(z)\n",
    "    z = Dense(256, activation='relu')(z)\n",
    "    z = Dropout(0.4)(z)\n",
    "    z = Dense(256, activation='relu')(z)\n",
    "    z = Dropout(0.4)(z)\n",
    "\n",
    "    class_out = Dense(1, activation='sigmoid')(z)\n",
    "    return Model(class_inputs, class_out, name='Classifier')\n",
    "\n",
    "def compile_classifier_stack(classifier, generator, loss, optimizer, metrics, loss_weights, name, reverse_freeze=False):\n",
    "    for layer in generator.layers: layer.trainable = False\n",
    "    generator.trainable = False\n",
    "    inp = Input(tuple(generator.layers[0].input_shape[1:]))\n",
    "    gen_out = [layer.output for layer in generator.layers if isinstance(layer, keras.layers.Add)][:8]\n",
    "    gen_multi_out = Model(generator.inputs, gen_out)\n",
    "    classifier_out = classifier(gen_multi_out(inp)+[inp])\n",
    "    model = Model(inp, classifier_out)\n",
    "    if reverse_freeze: model.layers[1].trainable = False\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics, loss_weights=loss_weights)\n",
    "    model.summary()\n",
    "    plot_model(model, show_shapes=True, to_file='DCGAN_model_classifier.png')\n",
    "    for layer in generator.layers: layer.trainable = True\n",
    "    generator.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X loaded\n",
      "(20000, 1, 20736, 1)\n",
      "Y loaded\n",
      "((20000, 1, 20736, 1), (20000, 1, 20736, 1))\n",
      "10089 (20000,)\n",
      "Params loaded\n",
      "Finished Loading Data\n",
      "loaded GAN\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed17d280597487d83fe31f561039ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20001), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.17505644, 1977.0, 1977.0, 20610)\n",
      "epoch: 0 current pic dice coeff: 0.175056\n",
      "8071/8071 [==============================] - 55s    \n",
      "((8071, 20610), (8071, 20610))\n",
      "(0.12905021, 11473636.0, 11473636.0, 166343310)\n",
      "epoch: 0 ,dice coef: 0.129050\n",
      "\n",
      "(0.036207717, 380.0, 380.0, 20610)\n",
      "epoch: 0 current pic dice coeff: 0.036208\n",
      "2018/2018 [==============================] - 13s    \n",
      "((2018, 20610), (2018, 20610))\n",
      "(0.13161066, 2929697.0, 2929697.0, 41590980)\n",
      "epoch: 0 ,dice coef: 0.131611\n",
      "\n",
      "(0.17505644, 1977.0, 1977.0, 20610)\n",
      "epoch: 100 current pic dice coeff: 0.175056\n",
      "8071/8071 [==============================] - 55s    \n",
      "((8071, 20610), (8071, 20610))\n",
      "(0.12905912, 11473635.0, 11473636.0, 166331324)\n",
      "epoch: 100 ,dice coef: 0.129059\n",
      "\n",
      "(0.036207717, 380.0, 380.0, 20610)\n",
      "epoch: 100 current pic dice coeff: 0.036208\n",
      "2018/2018 [==============================] - 13s    \n",
      "((2018, 20610), (2018, 20610))\n",
      "(0.13161939, 2929696.0, 2929697.0, 41588021)\n",
      "epoch: 100 ,dice coef: 0.131619\n",
      "\n",
      "(0.48585197, 1966.0, 1977.0, 6116)\n",
      "epoch: 200 current pic dice coeff: 0.485852\n",
      "8071/8071 [==============================] - 55s    \n",
      "((8071, 20610), (8071, 20610))\n",
      "(0.47693825, 10868326.0, 11473636.0, 34101699)\n",
      "epoch: 200 ,dice coef: 0.476938\n",
      "\n",
      "(0.18797922, 380.0, 380.0, 3663)\n",
      "epoch: 200 current pic dice coeff: 0.187979\n",
      "2018/2018 [==============================] - 13s    \n",
      "((2018, 20610), (2018, 20610))\n",
      "(0.48246971, 2775800.0, 2929697.0, 8576932)\n",
      "epoch: 200 ,dice coef: 0.482470\n",
      "\n",
      "(0.62614566, 1298.0, 1977.0, 2169)\n",
      "epoch: 300 current pic dice coeff: 0.626146\n",
      "2880/8071 [=========>....................] - ETA: 35s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fd9aed876647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m model.fit(x_train, y_train, steps=20001, batch_size=32, shuffle=True,\n\u001b[1;32m     53\u001b[0m           \u001b[0mgenerator_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlog_callback_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_callback_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissinglink_callback_gen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m           discriminator_callbacks=[schedule, missinglink_callback_dis])\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-5d0507caabfa>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, verbose, shuffle, steps, epochs, steps_per_epoch, batch_size, generator_training_multiplier, discriminator_training_multiplier, generator_callbacks, discriminator_callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                                                                                        \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                                                                                        \u001b[0mgenerator_training_multiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator_training_multiplier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                                                                                        discriminator_training_multiplier=discriminator_training_multiplier)\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0mloss_discriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'discriminator_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_loss_discriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0mloss_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generator_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_loss_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-5d0507caabfa>\u001b[0m in \u001b[0;36m__fit_on_batch\u001b[0;34m(self, x, y, step, epoch, steps_per_epoch, generator_training_multiplier, discriminator_training_multiplier, batch_size, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks_discriminator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# callbacks for discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b528bbe637f5>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"epoch: %d current pic dice coeff: %f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdice_coef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyslice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m20610\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m20610\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b528bbe637f5>\u001b[0m in \u001b[0;36mget_score\u001b[0;34m(self, i_in, gen_test)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgen_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mtransits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20610\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0mtransits_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20610\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransits_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/elad/anaconda2/envs/TF-1.1/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1713\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/elad/anaconda2/envs/TF-1.1/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/elad/anaconda2/envs/TF-1.1/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/elad/anaconda2/envs/TF-1.1/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/elad/anaconda2/envs/TF-1.1/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/elad/anaconda2/envs/TF-1.1/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/elad/anaconda2/envs/TF-1.1/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/elad/anaconda2/envs/TF-1.1/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!rm -rf ./TrainingLogs/FCN-SE/img_*.png\n",
    "\n",
    "def Discriminator_kwargs(batch_size):\n",
    "    Discriminator_training_kwargs = {\"optimizer\":Adam(1e-3, beta_1=0.5, beta_2=0.9),\n",
    "                                     \"batch_size\":batch_size} \n",
    "    return Discriminator_training_kwargs\n",
    "\n",
    "\n",
    "def Generator_kwargs():\n",
    "    Generator_training_kwargs = {\"gen_loss\":intersection_loss, \n",
    "                                 \"dis_loss\":IWGAN_TrainingScheme.wasserstein_loss,\n",
    "                                 \"optimizer\":Adam(1e-3, beta_1=0.5, beta_2=0.9), \n",
    "                                 \"dis_metrics\":['accuracy'],\n",
    "                                 \"gen_metrics\":[dice_coef1, masked_mse], \n",
    "                                 \"gen_dis_loss_ratio\":0.85} \n",
    "    return Generator_training_kwargs\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_simulation_data(only_true=True, cross_validation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./TrainingLogs/FCN-SE/img_*.png\n",
    "!rm -rf ./TrainingLogs/FCN/img_*.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAN(generator=Generator(x_train.shape[1:]), \n",
    "            discriminator=Discriminator(x_train.shape[1:]), \n",
    "            training_scheme=IWGAN_TrainingScheme,\n",
    "            generator_kwargs=Generator_kwargs(), \n",
    "            discriminator_kwargs=Discriminator_kwargs(batch_size=32), \n",
    "            generator_training_kwargs={}, \n",
    "            discriminator_training_kwargs={})\n",
    "print 'loaded GAN'\n",
    "#model.summaries()\n",
    "\n",
    "#classifier = compile_classifier(Classifier(), model.generator_model(), 'binary_crossentropy', Adam(), ['accuracy'], None, 'classifier', reverse_freeze=True)\n",
    "#print 'loaded Classifier'\n",
    "#class_train = classifier_training(classifier, 1, 32)\n",
    "log_callback_train = log_results_multi(100, 'best_model_train_resnet_se.txt', network_name='FCN-SE', \n",
    "                                       x=x_train, y=y_train, save_path='best_model_train_resnet_se.h5', image_name='train')\n",
    "log_callback_test = log_results_multi(100, 'best_model_test_resnet_se.txt', network_name='FCN-SE', \n",
    "                                      x=x_test, y=y_test, save_path='best_model_test_resnet_se.h5', image_name='test')\n",
    "def sch(epoch):\n",
    "    if epoch < 750:\n",
    "        return 1e-3\n",
    "    \n",
    "    if epoch < 1500:\n",
    "        if epoch == 750: print(\"changed lr to 5e-5\")\n",
    "        return 5e-5 \n",
    "    \n",
    "    if epoch == 1500: print(\"changed lr to 1e-5\")\n",
    "    return 1e-5\n",
    "import missinglink\n",
    "missinglink_callback_gen = missinglink.KerasCallback()\n",
    "missinglink_callback_gen.set_properties(display_name='generator test')\n",
    "missinglink_callback_dis = missinglink.KerasCallback()    \n",
    "missinglink_callback_dis.set_properties(display_name='discriminator test')\n",
    "schedule = keras.callbacks.LearningRateScheduler(sch)\n",
    "model.fit(x_train, y_train, steps=20001, batch_size=32, shuffle=True,\n",
    "          generator_callbacks=[log_callback_train, log_callback_test, schedule, missinglink_callback_gen],\n",
    "          discriminator_callbacks=[schedule, missinglink_callback_dis])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TF 1.1.0, Keras 2.0.8)",
   "language": "python",
   "name": "tf-1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
