{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and TF setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras \n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from scipy import signal, fftpack\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import UpSampling2D, LeakyReLU, Lambda, Add, Multiply, Activation, Conv2DTranspose\n",
    "from keras.layers import Cropping2D, ZeroPadding2D, Flatten, Subtract\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from functools import partial\n",
    "from New_Layers import *\n",
    "from keras.layers.merge import _Merge\n",
    "from multiprocessing import Pool\n",
    "import copy \n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import astropy\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "from astropy.io.fits.card import UNDEFINED\n",
    "from astropy.io import fits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.visible_device_list = \"0\"\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "# GAN Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, generator, discriminator, training_scheme, \n",
    "                 generator_kwargs={}, discriminator_kwargs={}, \n",
    "                 generator_training_kwargs={}, discriminator_training_kwargs={}):\n",
    "        \n",
    "        assert training_scheme is not None , \"No training scheme selected!\"\n",
    "        assert isinstance(generator, keras.models.Model), \"Generator is not a model!\"\n",
    "        assert isinstance(discriminator, keras.models.Model), \"Discriminator is not a model!\"\n",
    "        \n",
    "        assert type(generator_kwargs) is dict , \"generator kwargs are not a dictionary!\"\n",
    "        assert type(discriminator_kwargs) is dict , \"discriminator kwargs are not a dictionary!\"\n",
    "        assert type(generator_training_kwargs) is dict , \"discriminator training kwargs are not a dictionary!\"\n",
    "        assert type(discriminator_training_kwargs) is dict , \"generator training kwargs are not a dictionary!\"\n",
    "        \n",
    "        \n",
    "        self._training_scheme = training_scheme\n",
    "\n",
    "        self.__generator_model = generator\n",
    "        self.__discriminator_model = discriminator\n",
    "\n",
    "        self.__discriminator = self._training_scheme.compile_discriminator(self.__generator_model, \n",
    "                                                                           self.__discriminator_model, \n",
    "                                                                           **discriminator_kwargs)\n",
    "        self.__generator = self._training_scheme.compile_generator(self.__generator_model, \n",
    "                                                                   self.__discriminator_model,\n",
    "                                                                   **generator_kwargs)\n",
    "        self.generator_training_kwargs = generator_training_kwargs\n",
    "        self.discriminator_training_kwargs = discriminator_training_kwargs\n",
    "    \n",
    "    def generator_model(self): return self.__generator_model\n",
    "    def discriminator_model(self): return self.__discriminator_model\n",
    "    def generator(self): return self.__generator\n",
    "    def discriminator(self): return self.__discriminator\n",
    "    def summaries(self):\n",
    "        print \"\\n\\n\\nGenerator Summary: \\n\"\n",
    "        self.__generator_model.summary()\n",
    "        plot_model(self.__generator_model, show_shapes=True, to_file='GAN_Generator_Model.png')\n",
    "        \n",
    "        print \"\\n\\n\\nDiscriminator Summary: \\n\"\n",
    "        self.__discriminator_model.summary()\n",
    "        plot_model(self.__discriminator_model, show_shapes=True, to_file='GAN_Discriminator_Model.png')\n",
    "        \n",
    "        print \"\\n\\n\\nGenerator Training Model Summary: \\n\"\n",
    "        self.__generator.summary()\n",
    "        plot_model(self.__generator, show_shapes=True, to_file='GAN_Generator_Training_Model.png')\n",
    "       \n",
    "        print \"\\n\\n\\nDiscriminator Training Model Summary: \\n\"\n",
    "        self.__discriminator.summary()\n",
    "        plot_model(self.__discriminator, show_shapes=True, to_file='GAN_Discriminator_Training_Model.png')\n",
    "   \n",
    "    def fit(self, x, y, verbose=False, shuffle=False, steps=None, epochs=None, steps_per_epoch=None, batch_size=None, \n",
    "            generator_training_multiplier=1, discriminator_training_multiplier=1, \n",
    "            generator_callbacks=[],discriminator_callbacks=[], **kwargs):\n",
    "        self.verbose = verbose\n",
    "        self.callbacks_generator, self.callbacks_discriminator= [], []\n",
    "        self.History = keras.callbacks.History()\n",
    "        self.shuffle = shuffle\n",
    "        assert (steps is None and epochs is not None and steps_per_epoch is not None) or \\\n",
    "               (steps is not None and epochs is None and steps_per_epoch is None), \"please supply either steps OR epochs and steps per epoch\"\n",
    "        \n",
    "        assert batch_size is not None, \"batch size is None, please provide batch size\"\n",
    "        try:\n",
    "            iterator = iter(generator_callbacks)            \n",
    "        except TypeError:\n",
    "            assert False, \"generator callbacks are not iterable!\"\n",
    "        \n",
    "        try:\n",
    "            iterator = iter(discriminator_callbacks)            \n",
    "        except TypeError:\n",
    "            assert False, \"discriminator callbacks are not iterable!\"\n",
    "        \n",
    "        for c in generator_callbacks:\n",
    "            c.set_model(self.__generator)\n",
    "            self.callbacks_generator.append(c)\n",
    "        \n",
    "        for c in discriminator_callbacks:\n",
    "            c.set_model(self.__discriminator)\n",
    "            self.callbacks_discriminator.append(c)\n",
    "        \n",
    "        \n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator + [self.History]:\n",
    "            callback.on_train_begin()\n",
    "        \n",
    "        \n",
    "        if steps is not None:\n",
    "            for i in tqdm(xrange(steps)):\n",
    "                    temp_loss_discriminator, temp_loss_generator = self.__fit_on_batch(x=x, y=y, step=i, \n",
    "                                                                                       shuffle=self.shuffle,\n",
    "                                                                                       batch_size=batch_size, \n",
    "                                                                                       generator_training_multiplier=generator_training_multiplier,\n",
    "                                                                                       discriminator_training_multiplier=discriminator_training_multiplier)\n",
    "                    loss_discriminator = {('discriminator_'+self.__discriminator.metrics_names[i]):item for i,item in enumerate(temp_loss_discriminator)}\n",
    "                    loss_generator = {('generator_'+self.__generator.metrics_names[i]):item for i,item in enumerate(temp_loss_generator)}\n",
    "                    losses = loss_discriminator.copy().update(loss_generator)\n",
    "                    self.History.on_epoch_end(i,losses) # populate history\n",
    "        elif steps_per_epoch is not None and epochs is not None:\n",
    "            for k in xrange(epochs):\n",
    "                for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "                    callback.on_epoch_begin(k)\n",
    "                loss_discriminator, loss_generator = None, None\n",
    "                for i in xrange(steps_per_epoch):\n",
    "                    temp_loss_discriminator, temp_loss_generator = self.__fit_on_batch(x=x, y=y, step=i, epoch=k, \n",
    "                                                                                       shuffle=self.shuffle,\n",
    "                                                                                       steps_per_epoch=steps_per_epoch, \n",
    "                                                                                       batch_size=batch_size,\n",
    "                                                                                       generator_training_multiplier=generator_training_multiplier,\n",
    "                                                                                       discriminator_training_multiplier=discriminator_training_multiplier)\n",
    "\n",
    "                    if loss_discriminator is None: loss_discriminator = temp_loss_discriminator\n",
    "                    elif hasattr(loss_discriminator, '__iter__'): loss_discriminator = [x+y for x,y in zip(loss_discriminator, temp_loss_discriminator)]\n",
    "                    else: loss_discriminator += temp_loss_discriminator\n",
    "                        \n",
    "                    if loss_generator is None: loss_generator = temp_loss_generator\n",
    "                    elif hasattr(loss_generator, '__iter__'): loss_generator = [x+y for x,y in zip(loss_generator, temp_loss_generator)]\n",
    "                    else: loss_generator += temp_loss_generator\n",
    "                \n",
    "                loss_discriminator = [item * 1.0/steps_per_epoch for item in loss_discriminator]\n",
    "                loss_generator = [item * 1.0/steps_per_epoch for item in loss_generator]\n",
    "                                    \n",
    "                for callback in self.callbacks_generator:\n",
    "                    callback.on_epoch_end(k, logs={self.__generator.metrics_names[i]:item for i,item in enumerate(loss_generator)})\n",
    "\n",
    "                for callback in self.callbacks_discriminator:\n",
    "                    callback.on_epoch_end(k, logs={self.__discriminator.metrics_names[i]:item for i,item in enumerate(loss_discriminator)})\n",
    "\n",
    "                loss_discriminator = {('discriminator_'+self.__discriminator.metrics_names[i]):item for i,item in enumerate(loss_discriminator)}\n",
    "                loss_generator = {('generator_'+self.__generator.metrics_names[i]):item for i,item in enumerate(loss_generator)}\n",
    "                losses = loss_discriminator.copy().update(loss_generator)\n",
    "                self.History.on_epoch_end(k,losses) # populate history\n",
    "                    \n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "            callback.on_train_end()\n",
    "\n",
    "        return self.History\n",
    "\n",
    "\n",
    "    def __fit_on_batch(self, x, y, step, epoch=None, steps_per_epoch=None, generator_training_multiplier=1, \n",
    "                       discriminator_training_multiplier=1, batch_size=None, shuffle=False, **kwargs):\n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "                if epoch is not None:\n",
    "                    callback.on_batch_begin(step, logs={'batch':step, 'size':batch_size})\n",
    "                else:\n",
    "                    callback.on_epoch_begin(step)\n",
    "                    \n",
    "        steps = step if epoch is None else step + (epoch * steps_per_epoch)\n",
    "        curr_x, curr_y = x, y\n",
    "\n",
    "        # Discriminator Training\n",
    "        loss = None\n",
    "        for j in range(discriminator_training_multiplier): \n",
    "            curr = (steps*discriminator_training_multiplier + j) * batch_size\n",
    "            nex = curr + batch_size\n",
    "            idxs = [i % curr_x.shape[0] for i in range(curr,nex)]\n",
    "            if shuffle: idxs = np.random.randint(0, curr_x.shape[0], size=batch_size)\n",
    "            train_x, train_y = curr_x[idxs], curr_y[idxs]\n",
    "            temp_loss = self._training_scheme.train_discriminator(self.__discriminator, train_x, train_y, batch_size, **self.discriminator_training_kwargs)\n",
    "            if loss is None: loss = temp_loss\n",
    "            elif hasattr(loss, '__iter__'): loss = [x+y for x,y in zip(loss, temp_loss)]\n",
    "            else: loss += temp_loss\n",
    "             \n",
    "        loss_discriminator = [item * 1.0/discriminator_training_multiplier for item in loss]\n",
    "        if self.verbose: print 'Discriminator Loss:', loss_discriminator\n",
    "        \n",
    "        # Generator Training\n",
    "        loss = None\n",
    "        for j in range(generator_training_multiplier): \n",
    "            curr = (steps*generator_training_multiplier + j) * batch_size\n",
    "            nex = curr + batch_size\n",
    "            idxs = [i % curr_x.shape[0] for i in range(curr,nex)]\n",
    "            if shuffle: idxs = np.random.randint(0, curr_x.shape[0], size=batch_size)\n",
    "            train_x, train_y = curr_x[idxs], curr_y[idxs]\n",
    "            temp_loss = self._training_scheme.train_generator(self.__generator, train_x, train_y, batch_size, **self.generator_training_kwargs)\n",
    "            if loss is None: loss = temp_loss\n",
    "            elif hasattr(loss, '__iter__'): loss = [x+y for x,y in zip(loss, temp_loss)]\n",
    "            else: loss += temp_loss\n",
    "            \n",
    "        loss_generator = [item * 1.0/generator_training_multiplier for item in loss]\n",
    "        if self.verbose: print 'Generator Loss:', loss_generator\n",
    "\n",
    "        for callback in self.callbacks_generator: # callbacks for generator\n",
    "            if epoch is not None:\n",
    "                callback.on_batch_end(step, logs={self.__generator.metrics_names[i]:item for i,item in enumerate(loss_generator)})\n",
    "            else:\n",
    "                callback.on_epoch_end(step, logs={self.__generator.metrics_names[i]:item for i,item in enumerate(loss_generator)})\n",
    "            \n",
    "        for callback in self.callbacks_discriminator: # callbacks for discriminator\n",
    "            if epoch is not None:\n",
    "                callback.on_batch_end(step, logs={self.__discriminator.metrics_names[i]:item for i,item in enumerate(loss_discriminator)})\n",
    "            else:\n",
    "                callback.on_epoch_end(step, logs={self.__discriminator.metrics_names[i]:item for i,item in enumerate(loss_discriminator)})\n",
    "\n",
    "        return loss_discriminator, loss_generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# IWGAN TrainingScheme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_TrainingScheme(object):\n",
    "    @staticmethod\n",
    "    def compile_discriminator(generator, discriminator, **kwargs):\n",
    "            raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_generator(generator, discriminator, **kwargs):\n",
    "            raise NotImplementedError(\"Please Implement this method\")\n",
    "\n",
    "    @staticmethod\n",
    "    def train_discriminator(discriminator, x, y, batch_size, **kwargs):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_generator(generator, x, y, batch_size, **kwargs):\n",
    "        raise NotImplementedError(\"Please Implement this method\")\n",
    "    \n",
    "class IWGAN_TrainingScheme(Base_TrainingScheme):\n",
    "\n",
    "    @staticmethod\n",
    "    def wasserstein_loss(y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_penalty_loss(y_true, y_pred, averaged_samples, gradient_penalty_weight):\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "    @staticmethod\n",
    "    class RandomWeightedAverage(_Merge):\n",
    "        def __init__(self, batch_size,**kwargs):\n",
    "            super(IWGAN_TrainingScheme.RandomWeightedAverage, self).__init__(**kwargs)\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "        def _merge_function(self, inputs):\n",
    "            weights = K.random_uniform((self.batch_size, 1, 1, 1))\n",
    "            return (weights * inputs[0]) + ((1 - weights) * inputs[1])\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_discriminator(generator, discriminator, optimizer, batch_size, **kwargs):\n",
    "        gp_weight = 10\n",
    "        for layer in generator.layers: layer.trainable = False\n",
    "        generator.trainable = False\n",
    "        inp = Input(tuple(generator.layers[0].input_shape[1:]))\n",
    "        in_gen = generator(inp)\n",
    "        in_real = Input(tuple(discriminator.layers[0].input_shape[1:]))\n",
    "        discriminator_output_from_generator = discriminator(in_gen)\n",
    "        discriminator_output_from_real_samples = discriminator(in_real)\n",
    "        averaged_samples = IWGAN_TrainingScheme.RandomWeightedAverage(batch_size)([in_real, in_gen])\n",
    "        averaged_samples_out = discriminator(averaged_samples)\n",
    "        partial_gp_loss = partial(IWGAN_TrainingScheme.gradient_penalty_loss, averaged_samples=averaged_samples,\n",
    "                                  gradient_penalty_weight=gp_weight)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "        in_gen.trainable = False\n",
    "\n",
    "        # ----- Discriminator -----\n",
    "        discriminator_model = Model(inputs=[in_real, inp], outputs=[discriminator_output_from_real_samples,\n",
    "                                                                    discriminator_output_from_generator,\n",
    "                                                                    averaged_samples_out])\n",
    "        discriminator_model.layers[1].trainable = False\n",
    "        discriminator_model.compile(optimizer=optimizer, loss=[IWGAN_TrainingScheme.wasserstein_loss,\n",
    "                                                               IWGAN_TrainingScheme.wasserstein_loss, partial_gp_loss])\n",
    "        # ----- Discriminator -----\n",
    "\n",
    "        for layer in generator.layers: layer.trainable = True\n",
    "        generator.trainable = True\n",
    "        return discriminator_model\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_generator(generator, discriminator, gen_loss, dis_loss, optimizer, \n",
    "                          gen_metrics, dis_metrics, gen_dis_loss_ratio, **kwargs):\n",
    "\n",
    "        \n",
    "        for layer in discriminator.layers: layer.trainable = False\n",
    "        discriminator.trainable = False\n",
    "        inp = Input(tuple(generator.layers[0].input_shape[1:]))\n",
    "        gen_out = generator(inp)\n",
    "\n",
    "        # ----- Generator -----\n",
    "        model = Model(inp, [gen_out,discriminator(generator(inp))])\n",
    "        model.layers[2].trainable = False\n",
    "        model.compile(loss={'discriminator': dis_loss, 'generator': gen_loss},\n",
    "                      optimizer= optimizer, metrics={'discriminator':dis_metrics, 'generator':gen_metrics},\n",
    "                      loss_weights={'discriminator': 1-gen_dis_loss_ratio, 'generator':gen_dis_loss_ratio})\n",
    "        # ----- Generator -----\n",
    "\n",
    "        for layer in discriminator.layers: layer.trainable = True\n",
    "        discriminator.trainable = True\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def train_discriminator(discriminator, x, y, batch_size, **kwargs):\n",
    "        # Discriminator Training\n",
    "        loss = discriminator.train_on_batch([y, x],  # inp_real, x\n",
    "                                            [np.ones((batch_size, 1), dtype=np.float32),  # discriminator_output_from_real_samples\n",
    "                                            -np.ones((batch_size, 1), dtype=np.float32),  # discriminator_output_from_generator\n",
    "                                            np.zeros((batch_size, 1), dtype=np.float32)])  # averaged_samples_out\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def train_generator(generator, x, y, batch_size, **kwargs):\n",
    "        # Generator Training\n",
    "        loss = generator.train_on_batch(x, [y, np.ones((batch_size, 1), dtype=np.float32)])            #x, [y , 1]\n",
    "                                 \n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Callbacks and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier_training(keras.callbacks.Callback):\n",
    "    def __init__(self, classifier_model, training_ratio, batch_size):\n",
    "        self.classifier_model = classifier_model\n",
    "        self.train_ratio = training_ratio\n",
    "        self.x, _ ,self.y = load_simulation_data()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss = None\n",
    "        for i in xrange(self.train_ratio):\n",
    "            idxs = np.random.randint(0, self.x.shape[0], size=self.batch_size)\n",
    "            loss = self.classifier_model.train_on_batch(self.x[idxs], self.y[idxs])\n",
    "        #print 'class loss: ', loss\n",
    "\n",
    "class log_results(keras.callbacks.Callback):\n",
    "    def __init__(self, wgan_model=None, classifier_model=None, logging_frequency=0, log_path=''):\n",
    "        self.logging_frequency = logging_frequency\n",
    "        self.log_path = log_path\n",
    "        self.model = wgan_model\n",
    "        self.classifier_model = classifier_model\n",
    "        self.last_best_classifier, self.last_best_generator, self.last_best_combined= 0, 0, 0\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.logging_frequency != 0 and epoch % self.logging_frequency == 0:\n",
    "            time = np.linspace(0, 28.625, 20610)\n",
    "            plt.figure(1, figsize=(10,10))\n",
    "            plt.subplot(211)\n",
    "            plt.scatter(time,  self.y[:1][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Simulation Output, \" + str(epoch))\n",
    "\n",
    "            final_res = self.model.generator().predict_on_batch(self.x[:1])\n",
    "            plt.subplot(212)\n",
    "            plt.scatter(time, final_res[0][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Neural Net Output,\"+ str(final_res[1][0,0]))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('img_'+str(epoch)+'_test.png')\n",
    "            plt.show()\n",
    "            self.get_score(epoch, self.model != None, self.classifier_model != None)\n",
    "    \n",
    "    \n",
    "    def get_score(self, i_in, gen_test = False, class_test = False):\n",
    "        def find_nearest(array, value):\n",
    "            array = np.asarray(array)\n",
    "            idx = (np.abs(array - value)).argmin()\n",
    "            return idx\n",
    "        \n",
    "        at_2perc = 1\n",
    "        if gen_test:\n",
    "            periods = np.load('../Data/total_params_sim_test_true_3.npy')[:,1]\n",
    "            transits_ref = np.load('../Data/total_transits_sim_test_true_3.npy')\n",
    "            x_test = np.expand_dims(np.load('../Data/total_x_sim_test_true_3.npy'),axis=1)\n",
    "            x_test = np.pad(x_test, ((0,0), (0,0) ,(0, 30+ 96), (0,0)), 'constant', constant_values=(0, 0))\n",
    "            print 'X loaded'\n",
    "            transits = self.model.generator().predict(x_test, verbose=1)[0][:, 0, :20610, :]\n",
    "            print \"Finished predicting data\"\n",
    "            transits_ref = transits_ref[:,:,0]\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaler.fit(transits_ref)\n",
    "            transits_ref = scaler.transform(transits_ref)\n",
    "            print(\"Finished loading data\")\n",
    "            periods = np.power(10, periods)\n",
    "            period_pred = []\n",
    "            model_preds = []\n",
    "            np.warnings.filterwarnings('ignore')\n",
    "            for i in tqdm(range(10000)): model_preds.append([transits[i, :, 0], periods[i], 1000, transits_ref[i, :]])\n",
    "            model_preds = np.asarray(imap_unordered_bar(process_transit, model_preds, 5))\n",
    "            auc_p, percentages, epsilon_range = p_epsilon_chart(model_preds[:, 0], model_preds[:, 1])\n",
    "            at_1perc = percentages[np.argmin(np.abs(epsilon_range - 0.01))]\n",
    "            at_2perc = percentages[np.argmin(np.abs(epsilon_range - 0.02))]\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(epsilon_range, percentages, label='Keras (area = {:.5f})'.format(auc_p))\n",
    "            plt.xlabel('epsilon')\n",
    "            plt.ylabel('period detection rate')\n",
    "            plt.title('Period ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xlim(0, 0.1)\n",
    "            plt.savefig('img_PAUC_'+str(i_in)+'_test.png')\n",
    "            plt.show()\n",
    "\n",
    "            if(at_2perc > self.last_best_generator): \n",
    "                self.model.generator().save('best_generator_test.h5')\n",
    "                self.classifier_model.save('best_classifier_test_generator_based.h5')\n",
    "                self.last_best_generator = at_2perc\n",
    "\n",
    "            if not class_test:\n",
    "                open(self.log_path,'a').write( 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc))\n",
    "                print 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc)\n",
    "\n",
    "        if class_test:\n",
    "            y_test = np.load('../Data/total_params_sim_test_3.npy')[:,1] > 0\n",
    "            x_test = np.expand_dims(np.load('../Data/total_x_sim_test_3.npy'),axis=1)\n",
    "            x_test = np.pad(x_test, ((0,0), (0,0) ,(0, 30+ 96), (0,0)), 'constant', constant_values=(0, 0))\n",
    "            y_pred = self.classifier_model.predict(x_test, verbose=1)[:,0]\n",
    "            fpr, tpr, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "            if(tpr[find_nearest(fpr, 0.01)] > self.last_best_classifier): \n",
    "                self.model.generator().save('best_generator_test_classifier_based.h5')\n",
    "                self.classifier_model.save('best_classifier_test.h5')\n",
    "                self.last_best_classifier = tpr[find_nearest(fpr, 0.01)]\n",
    "            \n",
    "            if(tpr[find_nearest(fpr, 0.01)] * at_2perc > self.last_best_combined):\n",
    "                self.model.generator().save('best_generator_combined_test.h5')\n",
    "                self.classifier_model.save('best_classifier_combined_test.h5')\n",
    "                self.last_best_combined = tpr[find_nearest(fpr, 0.01)] * at_2perc              \n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(fpr, tpr, label='Keras (area = {:.5f})'.format(roc_auc))\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.title('Classifier ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xlim(0, 0.02)\n",
    "            plt.savefig('img_ClassAUC_'+str(i_in)+'_test.png')\n",
    "            plt.show()\n",
    "\n",
    "            if not gen_test:\n",
    "                open(self.log_path,'a').write( 'i: %d, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc))\n",
    "                print 'i: %d, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc)\n",
    "                \n",
    "        if class_test and gen_test:\n",
    "            open(self.log_path,'a').write( 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc))\n",
    "            print 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc)\n",
    "\n",
    "class RocAucMetricCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, predict_batch_size=80, include_on_batch=False):\n",
    "        super(RocAucMetricCallback, self).__init__()\n",
    "        self.predict_batch_size = predict_batch_size\n",
    "        self.include_on_batch = include_on_batch\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('roc_auc_val' in self.params['metrics']):\n",
    "            self.params['metrics'].append('roc_auc_val')\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['roc_auc_val'] = float('-inf')\n",
    "        if (self.validation_data):\n",
    "            y_pred = self.model.predict(self.validation_data[0])[:, 0]\n",
    "            y_test = self.validation_data[1]\n",
    "            fpr, tpr, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "            logs['roc_auc_val'] = roc_auc_score(y_test, y_pred)\n",
    "            print(logs['roc_auc_val'])\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(fpr, tpr, label='Keras (area = {:.5f})'.format(logs['roc_auc_val']))\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.title('ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xlim(0, 0.02)\n",
    "            plt.show()\n",
    "            def find_nearest(array, value):\n",
    "                array = np.asarray(array)\n",
    "                idx = (np.abs(array - value)).argmin()\n",
    "                return idx\n",
    "            print 'Percentage at 0.001: %.5f, Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], logs['roc_auc_val'])\n",
    "\n",
    "class log_results_multi(keras.callbacks.Callback):\n",
    "    def __init__(self, logging_frequency=0, log_path='', x=None, y=None, \n",
    "                 myslice=slice(0,1), network_name='', save_path='best_generator_test_sectors.h5', image_name=''):\n",
    "        self.LOGDIR = 'TrainingLogs'\n",
    "        self.network_name = network_name\n",
    "        if not os.path.exists(self.LOGDIR): os.makedirs(self.LOGDIR)\n",
    "        if not os.path.exists(os.path.join(self.LOGDIR, self.network_name)): os.makedirs(os.path.join(self.LOGDIR, self.network_name))\n",
    "        self.logging_frequency = logging_frequency\n",
    "        self.log_path = log_path\n",
    "        self.model = None\n",
    "        self.save_path = save_path\n",
    "        self.x_test, self.y_test = x, y \n",
    "        self.last_best_generator= 0\n",
    "        self.myslice = myslice\n",
    "        self.image_name = image_name\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.logging_frequency != 0 and epoch % self.logging_frequency == 0:\n",
    "            time = np.linspace(0, 28.625, 20610)\n",
    "            plt.figure(1, figsize=(15,10))\n",
    "            plt.subplot(311)\n",
    "            plt.scatter(time,  self.x_test[self.myslice][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Simulation Output, \" + str(epoch))\n",
    "\n",
    "            plt.subplot(312)\n",
    "            plt.scatter(time,  self.y_test[self.myslice][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Simulation Output, \" + str(epoch))\n",
    "\n",
    "            final_res = self.model.predict_on_batch(self.x_test[self.myslice])\n",
    "            plt.subplot(313)\n",
    "            plt.scatter(time, final_res[0][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Neural Net Output,\"+ str(final_res[1][0,0]))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.LOGDIR, self.network_name, 'img_' + str(epoch)+ '_' + self.image_name +'_test.png'))\n",
    "            #plt.show()\n",
    "            plt.clf()\n",
    "            plt.close()\n",
    "            print \"epoch: %d current pic dice coeff: %f\" %(epoch,dice_coef(self.y_test[self.myslice][0, 0, :20610, 0], final_res[0][0, 0, :20610, 0]))\n",
    "            self.get_score(epoch, self.model != None)\n",
    "    \n",
    "    \n",
    "    def get_score(self, i_in, gen_test = False):\n",
    "        def find_nearest(array, value):\n",
    "            array = np.asarray(array)\n",
    "            idx = (np.abs(array - value)).argmin()\n",
    "            return idx\n",
    "        \n",
    "        if gen_test:\n",
    "            transits = self.model.predict(self.x_test, verbose=1)[0][:,0,:20610,0]\n",
    "            transits_ref = self.y_test[:,0,:20610,0]\n",
    "            print(transits.shape, transits_ref.shape)\n",
    "            np.warnings.filterwarnings('ignore')\n",
    "            dice_coeff = dice_coef(transits_ref, transits)\n",
    "            \n",
    "            if(dice_coeff > self.last_best_generator): \n",
    "                self.model.save(os.path.join(self.LOGDIR, self.network_name, self.save_path))\n",
    "                self.last_best_generator = dice_coeff \n",
    "            open(os.path.join(self.LOGDIR, self.network_name, self.log_path),'a').write('epoch: %d ,dice coef: %f\\n' % (i_in, dice_coeff))\n",
    "            print('epoch: %d ,dice coef: %f\\n' % (i_in, dice_coeff))\n",
    "\n",
    "            \n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def dice_coef(y_true,y_pred):\n",
    "    y_pred = y_pred > 0.01\n",
    "    oyt = np.sum((2.0 * y_true * y_pred)) / np.sum(y_true+y_pred)\n",
    "    print(oyt, np.sum((y_true * y_pred)), np.sum(y_true), np.sum(y_pred))\n",
    "    return oyt#sum((2 * y_true * y_pred))/ sum(y_true+y_pred)\n",
    "\n",
    "def p_epsilon_chart(p_test, p_pred):\n",
    "    percentages = []\n",
    "    auc_p = 0\n",
    "    epsilon_range = np.linspace(0, 1, 10000)\n",
    "    for epsilon in epsilon_range:\n",
    "        current_correct = p_pred[np.abs(1 - (p_pred / p_test)) < epsilon]\n",
    "        percentages.append(float(current_correct.shape[0]) / float(p_pred.shape[0]))\n",
    "        auc_p += float(percentages[-1]) / 10000\n",
    "    return auc_p, percentages, epsilon_range\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# General Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_loss(y_true, y_pred):\n",
    "    return 10 * absolute_true_error(y_true, y_pred) + 3 * intersection_true_error(y_true, y_pred) + absolute_false_error(y_true, y_pred) + intersection_false_error(y_true, y_pred) - dice_coef1(y_true, y_pred)  # + masked_mse(y_true, y_pred)/10\n",
    "\n",
    "def load_simulation_data(header='', only_true=False, cross_validation=False):\n",
    "    x = np.load('../Data/total_x_sim_train_3.npy')\n",
    "    x = np.expand_dims(x,axis=1)\n",
    "    x = np.pad(x, ((0, 0), (0, 0), (0, 126), (0, 0)), 'constant', constant_values=(0.0, 0.0))\n",
    "    print 'X loaded'\n",
    "    y = np.load('../Data/total_transits_sim_train_3.npy')\n",
    "    y = y[:, :, 0]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(y)\n",
    "    y = scaler.transform(y)\n",
    "    y = np.expand_dims(y, axis=2)\n",
    "    ya = y[:, :, 0]\n",
    "    ya[ya != 0] = 1\n",
    "    y = np.expand_dims(ya, axis=2)\n",
    "    y = np.expand_dims(y, axis=1)\n",
    "    y = np.pad(y, ((0, 0), (0, 0), (0, 30 + 96), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "    print y.shape\n",
    "    print 'Y loaded'\n",
    "    print(x.shape, y.shape)\n",
    "    has_transit = np.load('../Data/total_params_sim_train_3.npy')[:,1] != 0\n",
    "    print np.sum(has_transit), has_transit.shape\n",
    "    print 'Params loaded'\n",
    "    print(\"Finished Loading Data\")\n",
    "    if only_true:\n",
    "        x = x[has_transit]\n",
    "        y = y[has_transit]\n",
    "    if cross_validation:\n",
    "        return train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "    return x, y, has_transit\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Network Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block_v2(inx, filters, kernel, activation, pooling):\n",
    "    x = BatchNormalization()(inx) # full pre activation\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = se_block(x, filters, 2) # Squeeze and Excitation Network\n",
    "    \n",
    "    x_k = add([x, inx]) # Resnet\n",
    "    \n",
    "    x = BatchNormalization()(x_k)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), strides=(1,pooling), padding='same')(x)\n",
    "    #x = MaxPooling2D(pool_size=(1, pooling), padding='same')(x) # convolutional Pool\n",
    "    return x, x_k # UNET\n",
    "\n",
    "def residual_block_up_v2(inx, x_k, filters, kernel, activation, pooling):\n",
    "    x = BatchNormalization()(inx) # full pre activation\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = se_block(x, filters, 2) # Squeeze and Excitation Network\n",
    "    x = add([x, x_k, inx]) # UNET with Resnet\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2DTranspose(filters, (1, kernel), strides=(1,pooling), padding='same')(x)\n",
    "    #x = UpSampling2D(size=(1, pooling))(x) # convolutional Pool\n",
    "    return x\n",
    "\n",
    "def residual_block(inx, filters, kernel, activation, pooling):\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(inx)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = add([x, inx])\n",
    "    x_k = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, pooling), strides=(1, pooling))(x_k)\n",
    "    return x, x_k\n",
    "\n",
    "def residual_block_up(inx, x_k, filters, kernel, activation, pooling):\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(inx)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = add([x, x_k, inx])\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = UpSampling2D((1, pooling))(x)\n",
    "    return x\n",
    "\n",
    "def se_block(inx, ch, ratio=16):\n",
    "    x = GlobalAveragePooling2D()(inx)\n",
    "    x = Dense(ch//ratio, activation='relu')(x)\n",
    "    x = Dense(ch, activation='sigmoid')(x)\n",
    "    return Multiply()([inx, x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(input_shape):\n",
    "    gen_inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (1, 5), padding='same')(gen_inputs)\n",
    "    x, x_k_1 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_2 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_3 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_4 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    \n",
    "    \n",
    "    x, x_k_5 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    x, x_k_6 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    x, x_k_7 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    y, x_k_8 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    \n",
    "    x = BatchNormalization()(y)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    decoder = x\n",
    "    x = Conv2D(64, (1, 5), padding='same')(decoder)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    m1 = x\n",
    "\n",
    "    x = Conv2D(128, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    m2 = x \n",
    "\n",
    "    x = Conv2D(256, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(256, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "\n",
    "    x = concatenate([m2,x])\n",
    "    x = Conv2D(128, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = concatenate([m1,x])\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = residual_block_up_v2(x, y, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_8, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_7, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_6, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_5, 64, 5, 'relu', 2)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (1, 5), padding='same')(x)\n",
    "    \n",
    "    x = residual_block_up_v2(x, x_k_4, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_3, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_2, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_1, 32, 5, 'relu', 1)\n",
    "    \n",
    "    _, x = residual_block_v2(x, 32, 5, 'relu', 1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    gen_outputs = Conv2D(1, (1, 1), activation='sigmoid', padding='same', name='gen_output')(x)\n",
    "    return Model(gen_inputs, gen_outputs, name='generator')\n",
    "    \n",
    "def Generator_2(input_shape):\n",
    "    gen_inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (1, 5), padding='same')(gen_inputs)\n",
    "    x, x_k_1 = residual_block(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_2 = residual_block(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_3 = residual_block(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_4 = residual_block(x, 32, 5, 'relu', 2)\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    x, x_k_5 = residual_block(x, 64, 5, 'relu', 2)\n",
    "    x, x_k_6 = residual_block(x, 64, 5, 'relu', 2)\n",
    "    x, x_k_7 = residual_block(x, 64, 5, 'relu', 2)\n",
    "    y, x_k_8 = residual_block(x, 64, 5, 'relu', 2)\n",
    "    x = Conv2D(64, (1, 5), padding='same')(y)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    encoded = x\n",
    "\n",
    "    decoder = encoded\n",
    "    x = Conv2D(64, (1, 5), padding='same')(decoder)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    m1 = x\n",
    "\n",
    "    x = Conv2D(128, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    m2 = x \n",
    "\n",
    "    x = Conv2D(256, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(256, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "\n",
    "    x = concatenate([m2,x])\n",
    "    x = Conv2D(128, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = concatenate([m1,x])\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = residual_block_up(x, y, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_8, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_7, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_6, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_5, 64, 5, 'relu', 2)\n",
    "    x = Conv2D(32, (1, 5), padding='same')(x)\n",
    "    x = residual_block_up(x, x_k_4, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_3, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_2, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up(x, x_k_1, 32, 5, 'relu', 1)\n",
    "    _, x = residual_block(x, 32, 5, 'relu', 1)\n",
    "    gen_outputs = Conv2D(1, (1, 1), activation='sigmoid', padding='same', name='gen_output')(x)\n",
    "    return Model(gen_inputs, gen_outputs, name='generator')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Discriminator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(input_shape):\n",
    "    dis_inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (1, 5), strides=(1,2), padding='same')(dis_inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(256, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    dis_outputs = Dense(1, kernel_initializer='he_normal', name='dis_output')(x)\n",
    "    return Model(dis_inputs, dis_outputs, name='discriminator')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Classifier Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Classifier(input_shape):\n",
    "    class_in = Input(shape=input_shape)\n",
    "    class_inputs = []\n",
    "    class_lengths = [32, 32, 32, 32, 64, 64, 64, 64]\n",
    "    for i in xrange(8):\n",
    "        class_inputs.append(Input(shape=(K.int_shape(gen_inputs)[1], K.int_shape(gen_inputs)[2]//(2**i), class_lengths[i])))\n",
    "    \n",
    "    z = Conv2D(32, (1, 5), padding='same')(class_in)\n",
    "    z = Add()([z, class_inputs[0]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 32, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[1]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 32, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[2]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 32, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[3]])\n",
    "\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Conv2D(64, (1, 5), padding='same')(z)\n",
    "\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[4]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[5]])\n",
    "    \n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[6]])\n",
    "    \n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[7]])\n",
    "    \n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    \n",
    "    z = BatchNormalization()(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Conv2D(128, (1, 5), kernel_initializer='he_normal', padding='same')(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = LeakyReLU(0.2)(z)\n",
    "    z = Dropout(0.25)(z)\n",
    "\n",
    "    z = Conv2D(256, (1, 5), kernel_initializer='he_normal', padding='same')(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = LeakyReLU(0.2)(z)\n",
    "    z = Dropout(0.25)(z)\n",
    "\n",
    "\n",
    "    z = GlobalMaxPooling2D()(z)\n",
    "    z = Dense(256, activation='relu')(z)\n",
    "    z = Dropout(0.4)(z)\n",
    "    z = Dense(256, activation='relu')(z)\n",
    "    z = Dropout(0.4)(z)\n",
    "\n",
    "    class_out = Dense(1, activation='sigmoid')(z)\n",
    "    return Model(class_inputs, class_out, name='Classifier')\n",
    "\n",
    "def compile_classifier_stack(classifier, generator, loss, optimizer, metrics, loss_weights, name, reverse_freeze=False):\n",
    "    for layer in generator.layers: layer.trainable = False\n",
    "    generator.trainable = False\n",
    "    inp = Input(tuple(generator.layers[0].input_shape[1:]))\n",
    "    gen_out = [layer.output for layer in generator.layers if isinstance(layer, keras.layers.Add)][:8]\n",
    "    gen_multi_out = Model(generator.inputs, gen_out)\n",
    "    classifier_out = classifier(gen_multi_out(inp)+[inp])\n",
    "    model = Model(inp, classifier_out)\n",
    "    if reverse_freeze: model.layers[1].trainable = False\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics, loss_weights=loss_weights)\n",
    "    model.summary()\n",
    "    plot_model(model, show_shapes=True, to_file='DCGAN_model_classifier.png')\n",
    "    for layer in generator.layers: layer.trainable = True\n",
    "    generator.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    " \n",
    "# Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./TrainingLogs/FCN-SE/img_*.png\n",
    "\n",
    "def Discriminator_kwargs(batch_size):\n",
    "    Discriminator_training_kwargs = {\"optimizer\":Adam(1e-3, beta_1=0.5, beta_2=0.9),\n",
    "                                     \"batch_size\":batch_size} \n",
    "    return Discriminator_training_kwargs\n",
    "\n",
    "\n",
    "def Generator_kwargs():\n",
    "    Generator_training_kwargs = {\"gen_loss\":intersection_loss, \n",
    "                                 \"dis_loss\":IWGAN_TrainingScheme.wasserstein_loss,\n",
    "                                 \"optimizer\":Adam(1e-3, beta_1=0.5, beta_2=0.9), \n",
    "                                 \"dis_metrics\":['accuracy'],\n",
    "                                 \"gen_metrics\":[dice_coef1, masked_mse], \n",
    "                                 \"gen_dis_loss_ratio\":0.85} \n",
    "    return Generator_training_kwargs\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_simulation_data(only_true=True, cross_validation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf ./TrainingLogs/FCN-SE/img_*.png\n",
    "#!rm -rf ./TrainingLogs/FCN/img_*.png\n",
    "model = GAN(generator=Generator(x_train.shape[1:]), \n",
    "            discriminator=Discriminator(x_train.shape[1:]), \n",
    "            training_scheme=IWGAN_TrainingScheme,\n",
    "            generator_kwargs=Generator_kwargs(), \n",
    "            discriminator_kwargs=Discriminator_kwargs(batch_size=32), \n",
    "            generator_training_kwargs={}, \n",
    "            discriminator_training_kwargs={})\n",
    "print 'loaded GAN'\n",
    "#model.summaries()\n",
    "\n",
    "#classifier = compile_classifier(Classifier(), model.generator_model(), 'binary_crossentropy', Adam(), ['accuracy'], None, 'classifier', reverse_freeze=True)\n",
    "#print 'loaded Classifier'\n",
    "#class_train = classifier_training(classifier, 1, 32)\n",
    "log_callback_train = log_results_multi(100, 'best_model_train_resnet_se.txt', network_name='FCN-SE', \n",
    "                                       x=x_train, y=y_train, save_path='best_model_train_resnet_se.h5', image_name='train')\n",
    "log_callback_test = log_results_multi(100, 'best_model_test_resnet_se.txt', network_name='FCN-SE', \n",
    "                                      x=x_test, y=y_test, save_path='best_model_test_resnet_se.h5', image_name='test')\n",
    "def sch(epoch):\n",
    "    if epoch < 750:\n",
    "        return 1e-3\n",
    "    \n",
    "    if epoch < 1500:\n",
    "        if epoch == 750: print(\"changed lr to 5e-5\")\n",
    "        return 5e-5 \n",
    "    \n",
    "    if epoch == 1500: print(\"changed lr to 1e-5\")\n",
    "    return 1e-5\n",
    "import missinglink\n",
    "missinglink_callback_gen = missinglink.KerasCallback()\n",
    "missinglink_callback_gen.set_properties(display_name='generator test')\n",
    "missinglink_callback_dis = missinglink.KerasCallback()    \n",
    "missinglink_callback_dis.set_properties(display_name='discriminator test')\n",
    "schedule = keras.callbacks.LearningRateScheduler(sch)\n",
    "model.fit(x_train, y_train, steps=20001, batch_size=32, shuffle=True,\n",
    "          generator_callbacks=[log_callback_train, log_callback_test, schedule, missinglink_callback_gen],\n",
    "          discriminator_callbacks=[schedule, missinglink_callback_dis])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[16A"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TF 1.1.0, Keras 2.0.8)",
   "language": "python",
   "name": "tf-1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
