{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras \n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from scipy import signal, fftpack\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, GlobalMaxPooling2D, UpSampling2D, LeakyReLU, Lambda, Add, Activation\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from functools import partial\n",
    "from New_Layers import *\n",
    "from keras.layers.merge import _Merge\n",
    "from multiprocessing import Pool\n",
    "import copy \n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import astropy\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "from astropy.io.fits.card import UNDEFINED\n",
    "from astropy.io import fits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.95\n",
    "config.gpu_options.visible_device_list = \"1\"\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "class IWGAN_TrainingScheme(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def wasserstein_loss(y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_penalty_loss(y_true, y_pred, averaged_samples, gradient_penalty_weight):\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        gradient_penalty = gradient_penalty_weight * K.square(1 - gradient_l2_norm)\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "    @staticmethod\n",
    "    class RandomWeightedAverage(_Merge):\n",
    "        def _merge_function(self, inputs):\n",
    "            weights = K.random_uniform((32, 1, 1, 1))\n",
    "            return (weights * inputs[0]) + ((1 - weights) * inputs[1])\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_discriminator(generator, discriminator, **kwargs):\n",
    "        optimizer = kwargs['optimizer']\n",
    "        gp_weight = 10\n",
    "        for layer in generator.layers: layer.trainable = False\n",
    "        generator.trainable = False\n",
    "        inp = Input(tuple(generator.layers[0].input_shape[1:]))\n",
    "        in_gen = generator(inp)\n",
    "        in_real = Input(tuple(discriminator.layers[0].input_shape[1:]))\n",
    "        discriminator_output_from_generator = discriminator(in_gen)\n",
    "        discriminator_output_from_real_samples = discriminator(in_real)\n",
    "        averaged_samples = IWGAN_TrainingScheme.RandomWeightedAverage()([in_real, in_gen])\n",
    "        averaged_samples_out = discriminator(averaged_samples)\n",
    "        partial_gp_loss = partial(IWGAN_TrainingScheme.gradient_penalty_loss, averaged_samples=averaged_samples,\n",
    "                                  gradient_penalty_weight=gp_weight)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "        in_gen.trainable = False\n",
    "\n",
    "        # ----- Discriminator -----\n",
    "        discriminator_model = Model(inputs=[in_real, inp], outputs=[discriminator_output_from_real_samples,\n",
    "                                                                    discriminator_output_from_generator,\n",
    "                                                                    averaged_samples_out])\n",
    "        discriminator_model.layers[1].trainable = False\n",
    "        discriminator_model.compile(optimizer=optimizer, loss=[IWGAN_TrainingScheme.wasserstein_loss,\n",
    "                                                               IWGAN_TrainingScheme.wasserstein_loss, partial_gp_loss])\n",
    "        # ----- Discriminator -----\n",
    "\n",
    "        for layer in generator.layers: layer.trainable = True\n",
    "        generator.trainable = True\n",
    "        plot_model(discriminator_model, show_shapes=True, to_file='DCGAN_model_dis.png')\n",
    "        return discriminator_model\n",
    "\n",
    "    @staticmethod\n",
    "    def compile_generator(generator, discriminator, **kwargs):\n",
    "        gen_loss = kwargs['gen_loss']\n",
    "        dis_loss = kwargs['dis_loss']\n",
    "        optimizer = kwargs['optimizer']\n",
    "        gen_metrics = kwargs['gen_metrics']\n",
    "        dis_metrics = kwargs['dis_metrics']\n",
    "        gen_dis_loss_ratio = kwargs['gen_dis_loss_ratio']\n",
    "        \n",
    "        \n",
    "        \n",
    "        for layer in discriminator.layers: layer.trainable = False\n",
    "        discriminator.trainable = False\n",
    "        inp = Input(tuple(generator.layers[0].input_shape[1:]))\n",
    "        gen_out = generator(inp)\n",
    "\n",
    "        # ----- Generator -----\n",
    "        model = Model(inp, [gen_out,discriminator(generator(inp))])\n",
    "        model.layers[2].trainable = False\n",
    "        model.compile(loss={'discriminator': kwargs['dis_loss'], 'generator': kwargs['gen_loss']},\n",
    "                      optimizer= kwargs['optimizer'], metrics={'discriminator':kwargs['dis_metrics'], 'generator':kwargs['gen_metrics']},\n",
    "                      loss_weights={'discriminator': 1-kwargs['gen_dis_loss_ratio'], 'generator':kwargs['gen_dis_loss_ratio']})\n",
    "        # ----- Generator -----\n",
    "\n",
    "        for layer in discriminator.layers: layer.trainable = True\n",
    "        discriminator.trainable = True\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def train_discriminator(discriminator, x, y, batch_size):\n",
    "        # Discriminator Training\n",
    "        loss = discriminator.train_on_batch([x, y],  # inp_real, x\n",
    "                                            [np.ones((batch_size, 1), dtype=np.float32),  # discriminator_output_from_real_samples\n",
    "                                            -np.ones((batch_size, 1), dtype=np.float32),  # discriminator_output_from_generator\n",
    "                                            np.zeros((batch_size, 1), dtype=np.float32)])  # averaged_samples_out\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def train_generator(generator, x, y, batch_size):\n",
    "\n",
    "        # Generator Training\n",
    "        loss = generator.train_on_batch(x, [y, np.ones((len(x_true), 1), dtype=np.float32)])            #x, [y , 1]\n",
    "                                 \n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class GAN(object):\n",
    "    def __init__(self, generator, discriminator, training_scheme, generator_kwargs={}, discriminator_kwargs={}, generator_training_kwargs={}, discriminator_training_kwargs={}):\n",
    "        \n",
    "        assert not training_scheme is None , \"No training scheme selected!\"\n",
    "        assert type(generator) is callable , \"No generator function supplied!\"\n",
    "        assert type(discriminator) is callable , \"No discriminator function supplied!\"\n",
    "        \n",
    "        assert type(generator_kwargs) is dict , \"generator kwargs are not a dictionary!\"\n",
    "        assert type(discriminator_kwargs) is dict , \"discriminator kwargs are not a dictionary!\"\n",
    "        assert type(generator_training_kwargs) is dict , \"discriminator training kwargs are not a dictionary!\"\n",
    "        assert type(discriminator_training_kwargs) is dict , \"generator training kwargs are not a dictionary!\"\n",
    "        \n",
    "        \n",
    "        self._training_scheme = training_scheme\n",
    "        # TODO: ----- Unrelated, Move out -----\n",
    "        self.__generator_model = generator(**generator_args) #Model(gen_inputs, gen_outputs, name='generator')\n",
    "        self.__discriminator_model = discriminator(**discriminator_args)#Model(dis_inputs, dis_outputs, name='discriminator')\n",
    "        # TODO: ----- Unrelated, Move out -----\n",
    "\n",
    "        self.__discriminator = self._training_scheme.compile_discriminator(self.__generator, self.__discriminator, **discriminator_training_kwargs)\n",
    "        self.__generator = self._training_scheme.compile_generator(self.__generator, self.__discriminator, **generator_training_kwargs)\n",
    "    \n",
    "    def generator_model(self): return self.__generator_model\n",
    "    def discriminator_model(self): return self.__discriminator_model\n",
    "    def generator(self): return self.__generator\n",
    "    def discriminator(self): return self.__discriminator\n",
    "    \n",
    "    def fit(self, x, y, verbose=True, steps=None, epochs=None, steps_per_epoch=None, batch_size=None, \n",
    "            generator_training_multiplier=1, discriminator_training_multiplier=1, \n",
    "            generator_callbacks=None,discriminator_callbacks=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.verbose = verbose\n",
    "        self.callbacks_generator, self.callbacks_discriminator= [], []\n",
    "        self.History = keras.callbacks.History()\n",
    "        \n",
    "        assert steps is None and not epochs is None and not steps_per_epoch is None, \"please supply either steps OR epochs and steps per epoch\"\n",
    "        assert not steps is None and epochs is None and steps_per_epoch is None, \"please supply either steps OR epochs and steps per epoch\"\n",
    "        \n",
    "        try:\n",
    "            iterator = iter(generator_callbacks)            \n",
    "        except TypeError:\n",
    "            assert False, \"generator callbacks are not iterable!\"\n",
    "        \n",
    "        try:\n",
    "            iterator = iter(discriminator_callbacks)            \n",
    "        except TypeError:\n",
    "            assert False, \"discriminator callbacks are not iterable!\"\n",
    "        \n",
    "        for c in generator_callbacks:\n",
    "            c.set_model(self.__generator)\n",
    "            self.callbacks_generator.append(c)\n",
    "        \n",
    "        for c in discriminator_callbacks:\n",
    "            c.set_model(self.__discriminator)\n",
    "            self.callbacks_discriminator.append(c)\n",
    "        \n",
    "        \n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator + [self.History]:\n",
    "            callback.on_train_begin()\n",
    "        \n",
    "        \n",
    "        if not steps is None:\n",
    "            for i in xrange(steps):\n",
    "                    temp_loss_discriminator, temp_loss_generator = self.__fit_on_batch(i, generator_training_multiplier,\n",
    "                                                                                       discriminator_training_multiplier, batch_size)\n",
    "                    loss_discriminator = {('discriminator_'+self.__discriminator.metrics_names[i]):item for i,item in enumerate(temp_loss_discriminator)}\n",
    "                    loss_generator = {('generator_'+self.__generator.metrics_names[i]):item for i,item in enumerate(temp_loss_generator)}\n",
    "                    self.History.on_epoch_end(i,loss_generator+loss_discriminator) # populate history\n",
    "        elif not steps_per_epoch is None and not epochs is None:\n",
    "            for k in xrange(epochs):\n",
    "                for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "                    callback.on_epoch_begin(k)\n",
    "                loss_discriminator, loss_generator = None, None\n",
    "                for i in xrange(steps_per_epoch):\n",
    "                    temp_loss_discriminator, temp_loss_generator = self.__fit_on_batch(i, k, steps_per_epoch,\n",
    "                                                                                       generator_training_multiplier,\n",
    "                                                                                       discriminator_training_multiplier, \n",
    "                                                                                       batch_size)\n",
    "                    loss_discriminator = temp_loss_discriminator if loss_discriminator is None else loss_discriminator + temp_loss_discriminator\n",
    "                    loss_generator = temp_loss_generator if loss_generator is None else loss_generator + temp_loss_generator\n",
    "                \n",
    "                loss_discriminator = [item * 1.0/steps_per_epoch for item in loss_discriminator]\n",
    "                loss_generator = [item * 1.0/steps_per_epoch for item in loss_generator]\n",
    "                                    \n",
    "                for callback in self.callbacks_generator:\n",
    "                    callback.on_epoch_end(k, logs={self.__generator.metrics_names[i]:item for i,item in enumerate(loss_generator)})\n",
    "\n",
    "                for callback in self.callbacks_discriminator:\n",
    "                    callback.on_epoch_end(k, logs={self.__discriminator.metrics_names[i]:item for i,item in enumerate(loss_discriminator)})\n",
    "\n",
    "                loss_discriminator = {('discriminator_'+self.__discriminator.metrics_names[i]):item for i,item in enumerate(loss_discriminator)}\n",
    "                loss_generator = {('generator_'+self.__generator.metrics_names[i]):item for i,item in enumerate(loss_generator)}\n",
    "                self.History.on_epoch_end(k,loss_generator+loss_discriminator) # populate history\n",
    "                    \n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "            callback.on_train_end()\n",
    "\n",
    "        return self.History\n",
    "\n",
    "\n",
    "    def __fit_on_batch(self, step, epoch=None, steps_per_epoch=None, generator_training_multiplier=1, discriminator_training_multiplier=1, batch_size=None):\n",
    "        for callback in self.callbacks_generator + self.callbacks_discriminator:\n",
    "                    callback.on_train_batch_begin(step, logs={'batch':step, 'size':batch_size})\n",
    "                \n",
    "        steps = step if epoch is None else step + (epoch * steps_per_epoch)\n",
    "\n",
    "        # Generator Training\n",
    "        loss = None\n",
    "        for j in xrange(generator_training_multiplier): \n",
    "            curr = (steps*generator_training_multiplier + j) * batch_size\n",
    "            nex = curr + batch_size\n",
    "            idxs = [i % self.x.shape[0] for i in range(curr,nex)]\n",
    "            temp_loss = self._training_scheme.train_generator(self.__generator, self.x[idxs], self.y[idxs], batch_size)\n",
    "            loss = temp_loss if loss is None else temp_loss + loss\n",
    "            \n",
    "        loss_generator = [item * 1.0/generator_training_multiplier for item in loss]\n",
    "        if self.verbose: print 'Generator Loss:', loss_generator\n",
    "\n",
    "        # Discriminator Training\n",
    "        loss = None\n",
    "        for j in xrange(discriminator_training_multiplier): \n",
    "            curr = (steps*discriminator_training_multiplier + j) * batch_size\n",
    "            nex = curr + batch_size\n",
    "            idxs = [i % self.x.shape[0] for i in range(curr,nex)]\n",
    "            temp_loss = self._training_scheme.train_discriminator(self.__discriminator, self.x[idxs], self.y[idxs], batch_size)\n",
    "            loss = temp_loss if loss is None else temp_loss + loss\n",
    "\n",
    "        loss_discriminator = [item * 1.0/discriminator_training_multiplier for item in loss]\n",
    "        if self.verbose: print 'Discriminator Loss:', loss_discriminator\n",
    "        \n",
    "        for callback in self.callbacks_generator: # callbacks for generator\n",
    "            callback.on_train_batch_end(step, logs={self.__generator.metrics_names[i]:item for i,item in enumerate(loss_generator)})\n",
    "            \n",
    "        for callback in self.callbacks_discriminator: # callbacks for discriminator\n",
    "            callback.on_train_batch_end(step, logs={self.__discriminator.metrics_names[i]:item for i,item in enumerate(loss_discriminator)})\n",
    "\n",
    "        return loss_discriminator, loss_generator\n",
    "    \n",
    "class classifier_training(keras.callbacks.Callback):\n",
    "    def __init__(self, classifier_model, training_ratio, batch_size):\n",
    "        self.classifier_model = classifier_model\n",
    "        self.train_ratio = training_ratio\n",
    "        self.x, _ ,self.y = load_simulation_data()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        loss = None\n",
    "        for i in xrange(self.train_ratio):\n",
    "            idxs = np.random.randint(0, self.x.shape[0], size=self.batch_size)\n",
    "            loss = self.classifier_model.train_on_batch(self.x[idxs], self.y[idxs])\n",
    "        #print 'class loss: ', loss\n",
    "\n",
    "class log_results(keras.callbacks.Callback):\n",
    "    def __init__(self, wgan_model=None, classifier_model=None, logging_frequency=0, log_path=''):\n",
    "        self.logging_frequency = logging_frequency\n",
    "        self.log_path = log_path\n",
    "        self.model = wgan_model\n",
    "        self.classifier_model = classifier_model\n",
    "        self.x, self.y, _ = load_simulation_data() \n",
    "        self.last_best_classifier, self.last_best_generator, self.last_best_combined= 0, 0, 0\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.logging_frequency != 0 and epoch % self.logging_frequency == 0:\n",
    "            time = np.linspace(0, 28.625, 20610)\n",
    "            plt.figure(1, figsize=(10,10))\n",
    "            plt.subplot(211)\n",
    "            plt.scatter(time,  self.y[:1][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Simulation Output, \" + str(epoch))\n",
    "\n",
    "            final_res = self.model.gen_dis().predict_on_batch(self.x[:1])\n",
    "            plt.subplot(212)\n",
    "            plt.scatter(time, final_res[0][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Neural Net Output,\"+ str(final_res[1][0,0]))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('img_'+str(epoch)+'_test.png')\n",
    "            plt.show()\n",
    "            self.get_score(epoch, self.model != None, self.classifier_model != None)\n",
    "    \n",
    "    \n",
    "    def get_score(self, i_in, gen_test = False, class_test = False):\n",
    "        def find_nearest(array, value):\n",
    "            array = np.asarray(array)\n",
    "            idx = (np.abs(array - value)).argmin()\n",
    "            return idx\n",
    "        \n",
    "        at_2perc = 1\n",
    "        if gen_test:\n",
    "            periods = np.load('../Data/total_params_sim_test_true_3.npy')[:,1]\n",
    "            transits_ref = np.load('../Data/total_transits_sim_test_true_3.npy')\n",
    "            x_test = np.expand_dims(np.load('../Data/total_x_sim_test_true_3.npy'),axis=1)\n",
    "            x_test = np.pad(x_test, ((0,0), (0,0) ,(0, 30+ 96), (0,0)), 'constant', constant_values=(0, 0))\n",
    "            print 'X loaded'\n",
    "            transits = self.model.gen_dis().predict(x_test, verbose=1)[0][:, 0, :20610, :]\n",
    "            print \"Finished predicting data\"\n",
    "            transits_ref = transits_ref[:,:,0]\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            scaler.fit(transits_ref)\n",
    "            transits_ref = scaler.transform(transits_ref)\n",
    "            print(\"Finished loading data\")\n",
    "            periods = np.power(10, periods)\n",
    "            period_pred = []\n",
    "            model_preds = []\n",
    "            np.warnings.filterwarnings('ignore')\n",
    "            for i in tqdm(range(10000)): model_preds.append([transits[i, :, 0], periods[i], 1000, transits_ref[i, :]])\n",
    "            model_preds = np.asarray(imap_unordered_bar(process_transit, model_preds, 5))\n",
    "            auc_p, percentages, epsilon_range = p_epsilon_chart(model_preds[:, 0], model_preds[:, 1])\n",
    "            at_1perc = percentages[np.argmin(np.abs(epsilon_range - 0.01))]\n",
    "            at_2perc = percentages[np.argmin(np.abs(epsilon_range - 0.02))]\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(epsilon_range, percentages, label='Keras (area = {:.5f})'.format(auc_p))\n",
    "            plt.xlabel('epsilon')\n",
    "            plt.ylabel('period detection rate')\n",
    "            plt.title('Period ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xlim(0, 0.1)\n",
    "            plt.savefig('img_PAUC_'+str(i_in)+'_test.png')\n",
    "            plt.show()\n",
    "\n",
    "            if(at_2perc > self.last_best_generator): \n",
    "                self.model.gen_dis().save('best_generator_test.h5')\n",
    "                self.classifier_model.save('best_classifier_test_generator_based.h5')\n",
    "                self.last_best_generator = at_2perc\n",
    "\n",
    "            if not class_test:\n",
    "                open(self.log_path,'a').write( 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc))\n",
    "                print 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc)\n",
    "\n",
    "        if class_test:\n",
    "            y_test = np.load('../Data/total_params_sim_test_3.npy')[:,1] > 0\n",
    "            x_test = np.expand_dims(np.load('../Data/total_x_sim_test_3.npy'),axis=1)\n",
    "            x_test = np.pad(x_test, ((0,0), (0,0) ,(0, 30+ 96), (0,0)), 'constant', constant_values=(0, 0))\n",
    "            y_pred = self.classifier_model.predict(x_test, verbose=1)[:,0]\n",
    "            fpr, tpr, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "            if(tpr[find_nearest(fpr, 0.01)] > self.last_best_classifier): \n",
    "                self.model.gen_dis().save('best_generator_test_classifier_based.h5')\n",
    "                self.classifier_model.save('best_classifier_test.h5')\n",
    "                self.last_best_classifier = tpr[find_nearest(fpr, 0.01)]\n",
    "            \n",
    "            if(tpr[find_nearest(fpr, 0.01)] * at_2perc > self.last_best_combined):\n",
    "                self.model.gen_dis().save('best_generator_combined_test.h5')\n",
    "                self.classifier_model.save('best_classifier_combined_test.h5')\n",
    "                self.last_best_combined = tpr[find_nearest(fpr, 0.01)] * at_2perc              \n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(fpr, tpr, label='Keras (area = {:.5f})'.format(roc_auc))\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.title('Classifier ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xlim(0, 0.02)\n",
    "            plt.savefig('img_ClassAUC_'+str(i_in)+'_test.png')\n",
    "            plt.show()\n",
    "\n",
    "            if not gen_test:\n",
    "                open(self.log_path,'a').write( 'i: %d, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc))\n",
    "                print 'i: %d, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc)\n",
    "                \n",
    "        if class_test and gen_test:\n",
    "            open(self.log_path,'a').write( 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc))\n",
    "            print 'i: %d, width: 1000, PAUC: %.5f, Percentage at 0.01: %.5f, Percentage at 0.02: %.5f, Class Percentage at 0.001: %.5f, Class Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (i_in,auc_p, at_1perc, at_2perc, tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], roc_auc)\n",
    "\n",
    "# ------------------------------ Anything below this line is unrelated to the GAN class ------------------------------\n",
    "\n",
    "\n",
    "# ---------------------------------------- Networks ----------------------------------------\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def dice_coef(y_true,y_pred):\n",
    "    y_pred = y_pred > 0.01\n",
    "    oyt = np.sum((2.0 * y_true * y_pred)) / np.sum(y_true+y_pred)\n",
    "    print(oyt, np.sum((y_true * y_pred)), np.sum(y_true), np.sum(y_pred))\n",
    "    return oyt#sum((2 * y_true * y_pred))/ sum(y_true+y_pred)\n",
    "\n",
    "def intersection_loss(y_true, y_pred):\n",
    "    return 10 * absolute_true_error(y_true, y_pred) + 3 * intersection_true_error(y_true, y_pred) + absolute_false_error(y_true, y_pred) + intersection_false_error(y_true, y_pred) - dice_coef1(y_true, y_pred)  # + masked_mse(y_true, y_pred)/10\n",
    "    #return - dice_coef1(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "def compile_classifier(classifier, generator, loss, optimizer, metrics, loss_weights, name, reverse_freeze=False):\n",
    "    for layer in generator.layers: layer.trainable = False\n",
    "    generator.trainable = False\n",
    "    inp = Input(tuple(generator.layers[0].input_shape[1:]))\n",
    "    gen_out = [layer.output for layer in generator.layers[0::8]][1:]\n",
    "    gen_multi_out = Model(generator.inputs, gen_out)\n",
    "    classifier_out = classifier(gen_multi_out(inp)+[inp])\n",
    "    model = Model(inp, classifier_out)\n",
    "    if reverse_freeze: model.layers[1].trainable = False\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics, loss_weights=loss_weights)\n",
    "    model.summary()\n",
    "    plot_model(model, show_shapes=True, to_file='DCGAN_model_classifier.png')\n",
    "    for layer in generator.layers: layer.trainable = True\n",
    "    generator.trainable = True\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------------------------------------- Utils ----------------------------------------\n",
    "\n",
    "class RocAucMetricCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, predict_batch_size=80, include_on_batch=False):\n",
    "        super(RocAucMetricCallback, self).__init__()\n",
    "        self.predict_batch_size = predict_batch_size\n",
    "        self.include_on_batch = include_on_batch\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        if not ('roc_auc_val' in self.params['metrics']):\n",
    "            self.params['metrics'].append('roc_auc_val')\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        logs['roc_auc_val'] = float('-inf')\n",
    "        if (self.validation_data):\n",
    "            y_pred = self.model.predict(self.validation_data[0])[:, 0]\n",
    "            y_test = self.validation_data[1]\n",
    "            fpr, tpr, thresholds_keras = roc_curve(y_test, y_pred)\n",
    "            logs['roc_auc_val'] = roc_auc_score(y_test, y_pred)\n",
    "            print(logs['roc_auc_val'])\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.plot(fpr, tpr, label='Keras (area = {:.5f})'.format(logs['roc_auc_val']))\n",
    "            plt.xlabel('False positive rate')\n",
    "            plt.ylabel('True positive rate')\n",
    "            plt.title('ROC curve')\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.xlim(0, 0.02)\n",
    "            plt.show()\n",
    "            def find_nearest(array, value):\n",
    "                array = np.asarray(array)\n",
    "                idx = (np.abs(array - value)).argmin()\n",
    "                return idx\n",
    "            print 'Percentage at 0.001: %.5f, Percentage at 0.01: %.5f, Class AUC: %.5f\\n' % (tpr[find_nearest(fpr, 0.001)], tpr[find_nearest(fpr, 0.01)], logs['roc_auc_val'])\n",
    "\n",
    "# ---------------------------------------- Utils ----------------------------------------\n",
    "\n",
    "\n",
    "# --------------------------------------------- STRuDL ---------------------------------------------\n",
    "\n",
    "def load_simulation_data(header='', batch_size=128, only_true=False, cross_validation=False):\n",
    "    x = np.load('../Data/total_x_sim_train_3.npy')\n",
    "    x = np.expand_dims(x,axis=1)\n",
    "    x = np.pad(x, ((0, 0), (0, 0), (0, 126), (0, 0)), 'constant', constant_values=(0.0, 0.0))\n",
    "    print 'X loaded'\n",
    "    y = np.load('../Data/total_transits_sim_train_3.npy')\n",
    "    y = y[:, :, 0]\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(y)\n",
    "    y = scaler.transform(y)\n",
    "    y = np.expand_dims(y, axis=2)\n",
    "    ya = y[:, :, 0]\n",
    "    ya[ya != 0] = 1\n",
    "    y = np.expand_dims(ya, axis=2)\n",
    "    y = np.expand_dims(y, axis=1)\n",
    "    y = np.pad(y, ((0, 0), (0, 0), (0, 30 + 96), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "    print y.shape\n",
    "    print 'Y loaded'\n",
    "    print(x.shape, y.shape)\n",
    "    has_transit = np.load('../Data/total_params_sim_train_3.npy')[:,1] != 0\n",
    "    print np.sum(has_transit), has_transit.shape\n",
    "    print 'Params loaded'\n",
    "    print(\"Finished Loading Data\")\n",
    "    if only_true:\n",
    "        x = x[has_transit]\n",
    "        y = y[has_transit]\n",
    "    if cross_validation:\n",
    "        return train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "    return x, y, has_transit\n",
    "\n",
    "def p_epsilon_chart(p_test, p_pred):\n",
    "    percentages = []\n",
    "    auc_p = 0\n",
    "    epsilon_range = np.linspace(0, 1, 10000)\n",
    "    for epsilon in epsilon_range:\n",
    "        current_correct = p_pred[np.abs(1 - (p_pred / p_test)) < epsilon]\n",
    "        percentages.append(float(current_correct.shape[0]) / float(p_pred.shape[0]))\n",
    "        auc_p += float(percentages[-1]) / 10000\n",
    "    return auc_p, percentages, epsilon_range\n",
    "\n",
    "# --------------------------------------------- STRuDL ---------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_data_now(header='', batch_size=128):\n",
    "    x = np.expand_dims(np.load('../Data/total_x_sim_3.npy'), axis=1)\n",
    "    x = np.pad(x, ((0, 0), (0, 0), (0, 30 + 96), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "    print 'X loaded'\n",
    "\n",
    "    params = np.load('../Data/total_params_sim_3.npy')\n",
    "    print 'Params loaded'\n",
    "    return x, 0, params\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block_v2(inx, filters, kernel, activation, pooling):\n",
    "    x = BatchNormalization()(inx) # full pre activation\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = se_block(x, filters, 4) # Squeeze and Excitation Network\n",
    "    \n",
    "    x_k = add([x, inx]) # Resnet\n",
    "    \n",
    "    x = BatchNormalization()(x_k)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), strides=(1, pooling))(x) # convolutional Pool\n",
    "    return x, x_k # UNET\n",
    "\n",
    "def residual_block_up_v2(inx, x_k, filters, kernel, activation, pooling):\n",
    "    x = BatchNormalization()(inx) # full pre activation\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = se_block(x, filters, 4) # Squeeze and Excitation Network\n",
    "    x = add([x, x_k, inx]) # UNET with Resnet\n",
    "    x = Conv2DTranspose(filters, (1, kernel), strides=(1, pooling), padding='same')(x) # deconvolutional network\n",
    "    return x\n",
    "\n",
    "def residual_block(inx, filters, kernel, activation, pooling):\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(inx)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = add([x, inx])\n",
    "    x_k = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, pooling), strides=(1, pooling))(x_k)\n",
    "    return x, x_k\n",
    "\n",
    "def residual_block_up(inx, x_k, filters, kernel, activation, pooling):\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(inx)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = Conv2D(filters, (1, kernel), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = add([x, x_k, inx])\n",
    "    x = Activation(activation=activation)(x)\n",
    "    x = UpSampling2D((1, pooling))(x)\n",
    "    return x\n",
    "\n",
    "def se_block(inx, ch, ratio=16):\n",
    "    x = GlobalAveragePooling2D()(inx)\n",
    "    x = Dense(ch//ratio, activation='relu')(x)\n",
    "    x = Dense(ch, activation='sigmoid')(x)\n",
    "    return multiply()([inx, x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(input_shape):\n",
    "    gen_inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(32, (1, 5), padding='same')(gen_inputs)\n",
    "    x, x_k_1 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_2 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_3 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    x, x_k_4 = residual_block_v2(x, 32, 5, 'relu', 2)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    \n",
    "    \n",
    "    x, x_k_5 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    x, x_k_6 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    x, x_k_7 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    y, x_k_8 = residual_block_v2(x, 64, 5, 'relu', 2)\n",
    "    \n",
    "    x = BatchNormalization()(y)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    decoder = x\n",
    "    x = Conv2D(64, (1, 5), padding='same')(decoder)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    m1 = x\n",
    "\n",
    "    x = Conv2D(128, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    m2 = x \n",
    "\n",
    "    x = Conv2D(256, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Dropout(0.25)(x)\n",
    "\n",
    "    x = Conv2D(256, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "\n",
    "    x = concatenate([m2,x])\n",
    "    x = Conv2D(128, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = concatenate([m1,x])\n",
    "    x = Conv2D(64, (1, 5), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = residual_block_up_v2(x, y, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_8, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_7, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_6, 64, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_5, 64, 5, 'relu', 2)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (1, 5), padding='same')(x)\n",
    "    \n",
    "    x = residual_block_up_v2(x, x_k_4, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_3, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_2, 32, 5, 'relu', 2)\n",
    "    x = residual_block_up_v2(x, x_k_1, 32, 5, 'relu', 1)\n",
    "    \n",
    "    _, x = residual_block_v2(x, 32, 5, 'relu', 1)\n",
    "    \n",
    "    gen_outputs = Conv2D(1, (1, 1), activation='sigmoid', padding='same', name='gen_output')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(input_shape):\n",
    "    dis_inputs = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (1, 5), strides=(1,2), padding='same')(dis_inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(64, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(128, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv2D(256, (1, 5), kernel_initializer='he_normal', strides=(1,2), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(0.2)(x)\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    dis_outputs = Dense(1, kernel_initializer='he_normal', name='dis_output')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier(input_shape):\n",
    "    class_in = Input(shape=input_shape)\n",
    "    class_inputs = []\n",
    "    class_lengths = [32, 32, 32, 32, 64, 64, 64, 64]\n",
    "    for i in xrange(8):\n",
    "        class_inputs.append(Input(shape=(K.int_shape(gen_inputs)[1], K.int_shape(gen_inputs)[2]//(2**i), class_lengths[i])))\n",
    "    \n",
    "    z = Conv2D(32, (1, 5), padding='same')(class_in)\n",
    "    z = Add()([z, class_inputs[0]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 32, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[1]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 32, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[2]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 32, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[3]])\n",
    "\n",
    "    z = BatchNormalization()(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Conv2D(64, (1, 5), padding='same')(z)\n",
    "\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[4]])\n",
    "\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[5]])\n",
    "    \n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[6]])\n",
    "    \n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z = Add()([z, class_inputs[7]])\n",
    "    \n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    z, _ = residual_block_v2(z, 64, 5, 'relu', 2)\n",
    "    \n",
    "    z = BatchNormalization()(z)\n",
    "    z = Activation('relu')(z)\n",
    "    z = Conv2D(128, (1, 5), kernel_initializer='he_normal', padding='same')(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = LeakyReLU(0.2)(z)\n",
    "    z = Dropout(0.25)(z)\n",
    "\n",
    "    z = Conv2D(256, (1, 5), kernel_initializer='he_normal', padding='same')(z)\n",
    "    z = BatchNormalization()(z)\n",
    "    z = LeakyReLU(0.2)(z)\n",
    "    z = Dropout(0.25)(z)\n",
    "\n",
    "\n",
    "    z = GlobalMaxPooling2D()(z)\n",
    "    z = Dense(256, activation='relu')(z)\n",
    "    z = Dropout(0.4)(z)\n",
    "    z = Dense(256, activation='relu')(z)\n",
    "    z = Dropout(0.4)(z)\n",
    "\n",
    "    class_out = Dense(1, activation='sigmoid')(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_results_multi(keras.callbacks.Callback):\n",
    "    def __init__(self, wgan_model=None, logging_frequency=0, log_path='', x_test=None, y_test=None, myslice=slice(0,1), network_name='', save_path='best_generator_test_sectors.h5'):\n",
    "        self.LOGDIR = 'TrainingLogs'\n",
    "        self.network_name = network_name\n",
    "        if not os.path.exists(self.LOGDIR): os.makedirs(self.LOGDIR)\n",
    "        if not os.path.exists(os.path.join(self.LOGDIR, self.network_name)): os.makedirs(os.path.join(self.LOGDIR, self.network_name))\n",
    "        self.logging_frequency = logging_frequency\n",
    "        self.log_path = log_path\n",
    "        self.model = wgan_model\n",
    "        self.save_path = save_path\n",
    "        self.x, self.y, _ = load_simulation_data() \n",
    "        self.x_test, self.y_test = x_test, y_test \n",
    "        self.last_best_generator, self.last_best_combined= 0, 0\n",
    "        self.myslice = myslice\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.logging_frequency != 0 and epoch % self.logging_frequency == 0:\n",
    "            time = np.linspace(0, 28.625, 20610)\n",
    "            plt.figure(1, figsize=(15,10))\n",
    "            plt.subplot(311)\n",
    "            plt.scatter(time,  self.x_test[self.myslice][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Simulation Output, \" + str(epoch))\n",
    "\n",
    "            plt.subplot(312)\n",
    "            plt.scatter(time,  self.y_test[self.myslice][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Simulation Output, \" + str(epoch))\n",
    "\n",
    "            final_res = self.model.gen_dis().predict_on_batch(self.x_test[self.myslice])\n",
    "            plt.subplot(313)\n",
    "            plt.scatter(time, final_res[0][0, 0, :20610, 0], s=0.5)\n",
    "            plt.title(\"Neural Net Output,\"+ str(final_res[1][0,0]))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.LOGDIR, self.network_name, 'img_' + str(epoch) + '_test.png'))\n",
    "            #plt.show()\n",
    "            plt.clf()\n",
    "            plt.close()\n",
    "            print \"epoch: %d current pic dice coeff: %f\" %(epoch,dice_coef(self.y_test[self.myslice][0, 0, :20610, 0], final_res[0][0, 0, :20610, 0]))\n",
    "            self.get_score(epoch, self.model != None)\n",
    "    \n",
    "    \n",
    "    def get_score(self, i_in, gen_test = False):\n",
    "        def find_nearest(array, value):\n",
    "            array = np.asarray(array)\n",
    "            idx = (np.abs(array - value)).argmin()\n",
    "            return idx\n",
    "        \n",
    "        if gen_test:\n",
    "            transits = self.model.gen_dis().predict(self.x_test, verbose=1)[0][:,0,:20610,0]\n",
    "            transits_ref = self.y_test[:,0,:20610,0]\n",
    "            print(transits.shape, transits_ref.shape)\n",
    "            np.warnings.filterwarnings('ignore')\n",
    "            dice_coeff = dice_coef(transits_ref, transits)\n",
    "            \n",
    "            if(dice_coeff > self.last_best_generator): \n",
    "                self.model.gen_dis().save(os.path.join(self.LOGDIR, self.network_name, self.save_path))\n",
    "                self.last_best_generator = dice_coeff \n",
    "            open(os.path.join(self.LOGDIR, self.network_name, self.log_path),'a').write('epoch: %d ,dice coef: %f\\n' % (i_in, dice_coeff))\n",
    "            print('epoch: %d ,dice coef: %f\\n' % (i_in, dice_coeff))\n",
    "\n",
    "#print 'loaded GAN'\n",
    "#model = GAN(gen_inputs, gen_outputs, Adam(1e-5, beta_1=0.5, beta_2=0.9), intersection_loss, dis_inputs, dis_outputs, class_in, class_out, dis_metrics=['accuracy'], gen_dis_loss_ratio = 0.75, gen_metrics=[dice_coef1, masked_mse])\n",
    "#model.gen_dis().load_weights('./best_generator_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SpaceGanFCN(object):\n",
    "    def __init__(self, gen_weights=None, class_weights=None, save_gen_weights=None, save_class_weights=None, lr=1e-4):\n",
    "        model = GAN(gen_inputs, gen_outputs, Adam(lr, beta_1=0.5, beta_2=0.9), intersection_loss, dis_inputs, dis_outputs, class_in, class_out, dis_metrics=['accuracy'], gen_dis_loss_ratio = 0.75, gen_metrics=[dice_coef1, masked_mse])    \n",
    "        if gen_weights: model.gen_dis().load_weights(gen_weights)        \n",
    "        print 'loaded GAN'\n",
    "        self.model = model\n",
    "        self.gen_dis = model.gen_dis\n",
    "        self.classifier = Model(class_inputs + [class_in], class_out, 'classifier')\n",
    "        self.classifier_model = compile_classifier(self.classifier, model.encoder(), 'binary_crossentropy', Adam(), ['accuracy'], None, 'classifier', reverse_freeze=True)\n",
    "        if class_weights: self.classifier_model.load_weights(class_weights)        \n",
    "            \n",
    "    def train(self, x_train, y_train, x_test, y_test, log_files=['sector_training.txt','sector_training.txt'], epochs=100001):\n",
    "        log_callback_train = log_results_multi(self.model, 100, log_files[0], network_name='FCN', x_test=x_train, y_test=y_train, save_path=log_files[0][:-4]+'.h5')\n",
    "        log_callback_test = log_results_multi(self.model, 100, log_files[1], network_name='FCN', x_test=x_test, y_test=y_test, save_path=log_files[1][:-4]+'.h5')\n",
    "        self.model.fit(x_train, y_train, gen_overtraining_multiplier=1, verbose=False, epochs=epochs, batch_size=32, callbacks=[log_callback_train, log_callback_test])    \n",
    "    \n",
    "    def generate_on_batch(self, batch):\n",
    "        return self.model.gen_dis().predict_on_batch(batch)\n",
    "\n",
    "    def generate(self, batch):\n",
    "        return self.model.gen_dis().predict(batch)\n",
    "        \n",
    "    def classify(self, batch):\n",
    "        return self.classifier_model.predict(batch)\n",
    "\n",
    "    def plot_prediction(self, x, y):\n",
    "        time = np.linspace(0, 28.625, 20610)\n",
    "        plt.figure(1, figsize=(10,10))\n",
    "        plt.subplot(311)\n",
    "        plt.scatter(time,  x[0, 0, :20610, 0], s=0.5)\n",
    "        plt.title(\"Simulation Output, \" + str(-1))\n",
    "\n",
    "        plt.subplot(312)\n",
    "        plt.scatter(time,  y[0, 0, :20610, 0], s=0.5)\n",
    "        plt.title(\"Simulation Output, \" + str(-1))\n",
    "\n",
    "        final_res = self.model.gen_dis().predict_on_batch(x)\n",
    "        plt.subplot(313)\n",
    "        plt.scatter(time, final_res[0][0, 0, :20610, 0], s=0.5)\n",
    "        plt.title(\"Neural Net Output,\"+ str(final_res[1][0,0]))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "# ---------- Tests ----------\n",
    "\n",
    "# x_train, y_train, x_test, y_test = load_nasa_tces()\n",
    "#x, y, has_transits = load_simulation_data(only_true=True)\n",
    "\n",
    "# model = SpaceGanFCN(gen_weights='./best_generator_test.h5')\n",
    "# model.train(x_train, y_train, x_test, y_test, log_file='sector_training.txt')\n",
    "# final_res = model.generate_on_batch(x_test[myslice])\n",
    "\n",
    "# model = SpaceGanFCN(gen_weights='./best_generator_test_sectors.h5', class_weights='best_classifier_test_generator_based.h5')\n",
    "# model.train(x_train, y_train, x_test, y_test, log_file='sector_training.txt')\n",
    "\n",
    "# ---------- Tests ----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TF 1.1.0, Keras 2.0.8)",
   "language": "python",
   "name": "tf-1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
